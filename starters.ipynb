{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "0d69b9c4-5f0e-4ee0-b074-932effef6471",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "841aa20c-25eb-473b-a22f-81324366a513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the data\n",
    "weight = 0.7\n",
    "bias = 0.3\n",
    "\n",
    "X = torch.arange(0,1, 0.02).unsqueeze(dim=1)\n",
    "y = weight * X + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "fefdb674-da0a-47b0-a696-f6cad46c7aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 40, 10, 10)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create train/test split\n",
    "train_split = int(0.8 * len(X)) # 80% of data used for training set, 20% for testing \n",
    "X_train, y_train = X[:train_split], y[:train_split]\n",
    "X_test, y_test = X[train_split:], y[train_split:]\n",
    "\n",
    "len(X_train), len(y_train), len(X_test), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "2120243a-23d9-4600-809d-b5d2e87b4ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(1,requires_grad = True,dtype = torch.float))\n",
    "        self.bias = nn.Parameter(torch.randn(1,requires_grad = True, dtype = torch.float))\n",
    "        \n",
    "    def forward(self,x:torch.Tensor)->torch.Tensor:\n",
    "        return self.weights * x + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "98ef08b5-aac0-4d4c-a623-7f1301363c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "modelOne = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b15f9f82-1aff-4d7e-8dde-1e382562a0da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([0.3367], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.1288], requires_grad=True)]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(modelOne.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "fee399f3-77bc-4986-a7ac-bec3a5e6659d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up loss function and optimmizer\n",
    "loss_fn = nn.L1Loss()\n",
    "optimizer = torch.optim.SGD(modelOne.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "65e4ece2-7b47-4ba1-b94b-862ae949f5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.31288138031959534\n",
      "Epoch: 0 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4931890368461609 \n",
      "Loss: 0.3117292523384094\n",
      "Epoch: 1 | MAE Train Loss: 0.3117292523384094 | MAE Test Loss: 0.4918419420719147 \n",
      "Loss: 0.3105771541595459\n",
      "Epoch: 2 | MAE Train Loss: 0.3105771541595459 | MAE Test Loss: 0.49049490690231323 \n",
      "Loss: 0.3094250559806824\n",
      "Epoch: 3 | MAE Train Loss: 0.3094250559806824 | MAE Test Loss: 0.48914775252342224 \n",
      "Loss: 0.30827295780181885\n",
      "Epoch: 4 | MAE Train Loss: 0.30827295780181885 | MAE Test Loss: 0.4878006875514984 \n",
      "Loss: 0.3071208596229553\n",
      "Epoch: 5 | MAE Train Loss: 0.3071208596229553 | MAE Test Loss: 0.4864535331726074 \n",
      "Loss: 0.3059687614440918\n",
      "Epoch: 6 | MAE Train Loss: 0.3059687614440918 | MAE Test Loss: 0.4851064682006836 \n",
      "Loss: 0.3048166334629059\n",
      "Epoch: 7 | MAE Train Loss: 0.3048166334629059 | MAE Test Loss: 0.48375946283340454 \n",
      "Loss: 0.30366456508636475\n",
      "Epoch: 8 | MAE Train Loss: 0.30366456508636475 | MAE Test Loss: 0.48241233825683594 \n",
      "Loss: 0.3025124669075012\n",
      "Epoch: 9 | MAE Train Loss: 0.3025124669075012 | MAE Test Loss: 0.48106521368026733 \n",
      "Loss: 0.3013603389263153\n",
      "Epoch: 10 | MAE Train Loss: 0.3013603389263153 | MAE Test Loss: 0.4797181189060211 \n",
      "Loss: 0.30020827054977417\n",
      "Epoch: 11 | MAE Train Loss: 0.30020827054977417 | MAE Test Loss: 0.4783710837364197 \n",
      "Loss: 0.29905614256858826\n",
      "Epoch: 12 | MAE Train Loss: 0.29905614256858826 | MAE Test Loss: 0.4770239293575287 \n",
      "Loss: 0.2979040741920471\n",
      "Epoch: 13 | MAE Train Loss: 0.2979040741920471 | MAE Test Loss: 0.47567683458328247 \n",
      "Loss: 0.2967519164085388\n",
      "Epoch: 14 | MAE Train Loss: 0.2967519164085388 | MAE Test Loss: 0.47432971000671387 \n",
      "Loss: 0.29559987783432007\n",
      "Epoch: 15 | MAE Train Loss: 0.29559987783432007 | MAE Test Loss: 0.47298264503479004 \n",
      "Loss: 0.29444774985313416\n",
      "Epoch: 16 | MAE Train Loss: 0.29444774985313416 | MAE Test Loss: 0.47163552045822144 \n",
      "Loss: 0.293295681476593\n",
      "Epoch: 17 | MAE Train Loss: 0.293295681476593 | MAE Test Loss: 0.4702884256839752 \n",
      "Loss: 0.2921435832977295\n",
      "Epoch: 18 | MAE Train Loss: 0.2921435832977295 | MAE Test Loss: 0.4689413607120514 \n",
      "Loss: 0.2909914553165436\n",
      "Epoch: 19 | MAE Train Loss: 0.2909914553165436 | MAE Test Loss: 0.4675942361354828 \n",
      "Loss: 0.28983938694000244\n",
      "Epoch: 20 | MAE Train Loss: 0.28983938694000244 | MAE Test Loss: 0.4662471413612366 \n",
      "Loss: 0.28868725895881653\n",
      "Epoch: 21 | MAE Train Loss: 0.28868725895881653 | MAE Test Loss: 0.46490007638931274 \n",
      "Loss: 0.287535160779953\n",
      "Epoch: 22 | MAE Train Loss: 0.287535160779953 | MAE Test Loss: 0.46355295181274414 \n",
      "Loss: 0.2863830626010895\n",
      "Epoch: 23 | MAE Train Loss: 0.2863830626010895 | MAE Test Loss: 0.4622058868408203 \n",
      "Loss: 0.28523099422454834\n",
      "Epoch: 24 | MAE Train Loss: 0.28523099422454834 | MAE Test Loss: 0.4608587324619293 \n",
      "Loss: 0.2840788662433624\n",
      "Epoch: 25 | MAE Train Loss: 0.2840788662433624 | MAE Test Loss: 0.4595116674900055 \n",
      "Loss: 0.2829267382621765\n",
      "Epoch: 26 | MAE Train Loss: 0.2829267382621765 | MAE Test Loss: 0.4581645429134369 \n",
      "Loss: 0.281774640083313\n",
      "Epoch: 27 | MAE Train Loss: 0.281774640083313 | MAE Test Loss: 0.45681753754615784 \n",
      "Loss: 0.28062257170677185\n",
      "Epoch: 28 | MAE Train Loss: 0.28062257170677185 | MAE Test Loss: 0.45547038316726685 \n",
      "Loss: 0.2794705033302307\n",
      "Epoch: 29 | MAE Train Loss: 0.2794705033302307 | MAE Test Loss: 0.454123318195343 \n",
      "Loss: 0.2783183455467224\n",
      "Epoch: 30 | MAE Train Loss: 0.2783183455467224 | MAE Test Loss: 0.4527761936187744 \n",
      "Loss: 0.2771662771701813\n",
      "Epoch: 31 | MAE Train Loss: 0.2771662771701813 | MAE Test Loss: 0.4514291286468506 \n",
      "Loss: 0.27601414918899536\n",
      "Epoch: 32 | MAE Train Loss: 0.27601414918899536 | MAE Test Loss: 0.450082004070282 \n",
      "Loss: 0.2748620808124542\n",
      "Epoch: 33 | MAE Train Loss: 0.2748620808124542 | MAE Test Loss: 0.44873490929603577 \n",
      "Loss: 0.2737099528312683\n",
      "Epoch: 34 | MAE Train Loss: 0.2737099528312683 | MAE Test Loss: 0.44738784432411194 \n",
      "Loss: 0.2725578844547272\n",
      "Epoch: 35 | MAE Train Loss: 0.2725578844547272 | MAE Test Loss: 0.44604071974754333 \n",
      "Loss: 0.27140578627586365\n",
      "Epoch: 36 | MAE Train Loss: 0.27140578627586365 | MAE Test Loss: 0.4446936249732971 \n",
      "Loss: 0.2702536880970001\n",
      "Epoch: 37 | MAE Train Loss: 0.2702536880970001 | MAE Test Loss: 0.4433465003967285 \n",
      "Loss: 0.2691015899181366\n",
      "Epoch: 38 | MAE Train Loss: 0.2691015899181366 | MAE Test Loss: 0.4419994354248047 \n",
      "Loss: 0.26794949173927307\n",
      "Epoch: 39 | MAE Train Loss: 0.26794949173927307 | MAE Test Loss: 0.44065237045288086 \n",
      "Loss: 0.26679736375808716\n",
      "Epoch: 40 | MAE Train Loss: 0.26679736375808716 | MAE Test Loss: 0.43930521607398987 \n",
      "Loss: 0.26564526557922363\n",
      "Epoch: 41 | MAE Train Loss: 0.26564526557922363 | MAE Test Loss: 0.43795815110206604 \n",
      "Loss: 0.2644931674003601\n",
      "Epoch: 42 | MAE Train Loss: 0.2644931674003601 | MAE Test Loss: 0.43661102652549744 \n",
      "Loss: 0.2633410692214966\n",
      "Epoch: 43 | MAE Train Loss: 0.2633410692214966 | MAE Test Loss: 0.4352639615535736 \n",
      "Loss: 0.26218897104263306\n",
      "Epoch: 44 | MAE Train Loss: 0.26218897104263306 | MAE Test Loss: 0.4339168071746826 \n",
      "Loss: 0.26103687286376953\n",
      "Epoch: 45 | MAE Train Loss: 0.26103687286376953 | MAE Test Loss: 0.43256980180740356 \n",
      "Loss: 0.259884774684906\n",
      "Epoch: 46 | MAE Train Loss: 0.259884774684906 | MAE Test Loss: 0.4312226176261902 \n",
      "Loss: 0.2587326467037201\n",
      "Epoch: 47 | MAE Train Loss: 0.2587326467037201 | MAE Test Loss: 0.42987555265426636 \n",
      "Loss: 0.25758057832717896\n",
      "Epoch: 48 | MAE Train Loss: 0.25758057832717896 | MAE Test Loss: 0.42852845788002014 \n",
      "Loss: 0.2564285099506378\n",
      "Epoch: 49 | MAE Train Loss: 0.2564285099506378 | MAE Test Loss: 0.4271813929080963 \n",
      "Loss: 0.2552763819694519\n",
      "Epoch: 50 | MAE Train Loss: 0.2552763819694519 | MAE Test Loss: 0.4258342385292053 \n",
      "Loss: 0.2541242837905884\n",
      "Epoch: 51 | MAE Train Loss: 0.2541242837905884 | MAE Test Loss: 0.4244872033596039 \n",
      "Loss: 0.25297218561172485\n",
      "Epoch: 52 | MAE Train Loss: 0.25297218561172485 | MAE Test Loss: 0.4231400489807129 \n",
      "Loss: 0.25182008743286133\n",
      "Epoch: 53 | MAE Train Loss: 0.25182008743286133 | MAE Test Loss: 0.42179298400878906 \n",
      "Loss: 0.2506679892539978\n",
      "Epoch: 54 | MAE Train Loss: 0.2506679892539978 | MAE Test Loss: 0.42044591903686523 \n",
      "Loss: 0.2495158612728119\n",
      "Epoch: 55 | MAE Train Loss: 0.2495158612728119 | MAE Test Loss: 0.41909879446029663 \n",
      "Loss: 0.24836377799510956\n",
      "Epoch: 56 | MAE Train Loss: 0.24836377799510956 | MAE Test Loss: 0.4177517294883728 \n",
      "Loss: 0.24721166491508484\n",
      "Epoch: 57 | MAE Train Loss: 0.24721166491508484 | MAE Test Loss: 0.4164046347141266 \n",
      "Loss: 0.2460596114397049\n",
      "Epoch: 58 | MAE Train Loss: 0.2460596114397049 | MAE Test Loss: 0.415057510137558 \n",
      "Loss: 0.2449074685573578\n",
      "Epoch: 59 | MAE Train Loss: 0.2449074685573578 | MAE Test Loss: 0.41371044516563416 \n",
      "Loss: 0.24375538527965546\n",
      "Epoch: 60 | MAE Train Loss: 0.24375538527965546 | MAE Test Loss: 0.41236335039138794 \n",
      "Loss: 0.24260330200195312\n",
      "Epoch: 61 | MAE Train Loss: 0.24260330200195312 | MAE Test Loss: 0.41101622581481934 \n",
      "Loss: 0.2414511889219284\n",
      "Epoch: 62 | MAE Train Loss: 0.2414511889219284 | MAE Test Loss: 0.40966910123825073 \n",
      "Loss: 0.2402990758419037\n",
      "Epoch: 63 | MAE Train Loss: 0.2402990758419037 | MAE Test Loss: 0.4083220064640045 \n",
      "Loss: 0.23914699256420135\n",
      "Epoch: 64 | MAE Train Loss: 0.23914699256420135 | MAE Test Loss: 0.4069749414920807 \n",
      "Loss: 0.23799486458301544\n",
      "Epoch: 65 | MAE Train Loss: 0.23799486458301544 | MAE Test Loss: 0.4056278169155121 \n",
      "Loss: 0.2368427962064743\n",
      "Epoch: 66 | MAE Train Loss: 0.2368427962064743 | MAE Test Loss: 0.40428075194358826 \n",
      "Loss: 0.23569071292877197\n",
      "Epoch: 67 | MAE Train Loss: 0.23569071292877197 | MAE Test Loss: 0.40293365716934204 \n",
      "Loss: 0.23453858494758606\n",
      "Epoch: 68 | MAE Train Loss: 0.23453858494758606 | MAE Test Loss: 0.4015865921974182 \n",
      "Loss: 0.23338651657104492\n",
      "Epoch: 69 | MAE Train Loss: 0.23338651657104492 | MAE Test Loss: 0.4002394676208496 \n",
      "Loss: 0.232234388589859\n",
      "Epoch: 70 | MAE Train Loss: 0.232234388589859 | MAE Test Loss: 0.398892343044281 \n",
      "Loss: 0.23108229041099548\n",
      "Epoch: 71 | MAE Train Loss: 0.23108229041099548 | MAE Test Loss: 0.3975452482700348 \n",
      "Loss: 0.22993019223213196\n",
      "Epoch: 72 | MAE Train Loss: 0.22993019223213196 | MAE Test Loss: 0.3961981534957886 \n",
      "Loss: 0.22877809405326843\n",
      "Epoch: 73 | MAE Train Loss: 0.22877809405326843 | MAE Test Loss: 0.39485102891921997 \n",
      "Loss: 0.2276259958744049\n",
      "Epoch: 74 | MAE Train Loss: 0.2276259958744049 | MAE Test Loss: 0.39350399374961853 \n",
      "Loss: 0.22647389769554138\n",
      "Epoch: 75 | MAE Train Loss: 0.22647389769554138 | MAE Test Loss: 0.3921568691730499 \n",
      "Loss: 0.22532181441783905\n",
      "Epoch: 76 | MAE Train Loss: 0.22532181441783905 | MAE Test Loss: 0.3908098042011261 \n",
      "Loss: 0.22416968643665314\n",
      "Epoch: 77 | MAE Train Loss: 0.22416968643665314 | MAE Test Loss: 0.3894627094268799 \n",
      "Loss: 0.223017618060112\n",
      "Epoch: 78 | MAE Train Loss: 0.223017618060112 | MAE Test Loss: 0.3881155848503113 \n",
      "Loss: 0.2218654900789261\n",
      "Epoch: 79 | MAE Train Loss: 0.2218654900789261 | MAE Test Loss: 0.38676849007606506 \n",
      "Loss: 0.22071340680122375\n",
      "Epoch: 80 | MAE Train Loss: 0.22071340680122375 | MAE Test Loss: 0.38542139530181885 \n",
      "Loss: 0.21956129372119904\n",
      "Epoch: 81 | MAE Train Loss: 0.21956129372119904 | MAE Test Loss: 0.38407427072525024 \n",
      "Loss: 0.2184092104434967\n",
      "Epoch: 82 | MAE Train Loss: 0.2184092104434967 | MAE Test Loss: 0.3827272057533264 \n",
      "Loss: 0.21725709736347198\n",
      "Epoch: 83 | MAE Train Loss: 0.21725709736347198 | MAE Test Loss: 0.3813800811767578 \n",
      "Loss: 0.21610502898693085\n",
      "Epoch: 84 | MAE Train Loss: 0.21610502898693085 | MAE Test Loss: 0.38003304600715637 \n",
      "Loss: 0.21495290100574493\n",
      "Epoch: 85 | MAE Train Loss: 0.21495290100574493 | MAE Test Loss: 0.3786858916282654 \n",
      "Loss: 0.2138008177280426\n",
      "Epoch: 86 | MAE Train Loss: 0.2138008177280426 | MAE Test Loss: 0.37733882665634155 \n",
      "Loss: 0.21264871954917908\n",
      "Epoch: 87 | MAE Train Loss: 0.21264871954917908 | MAE Test Loss: 0.37599173188209534 \n",
      "Loss: 0.21149662137031555\n",
      "Epoch: 88 | MAE Train Loss: 0.21149662137031555 | MAE Test Loss: 0.3746446371078491 \n",
      "Loss: 0.21034450829029083\n",
      "Epoch: 89 | MAE Train Loss: 0.21034450829029083 | MAE Test Loss: 0.3732975423336029 \n",
      "Loss: 0.2091923952102661\n",
      "Epoch: 90 | MAE Train Loss: 0.2091923952102661 | MAE Test Loss: 0.3719504475593567 \n",
      "Loss: 0.20804031193256378\n",
      "Epoch: 91 | MAE Train Loss: 0.20804031193256378 | MAE Test Loss: 0.3706033527851105 \n",
      "Loss: 0.20688819885253906\n",
      "Epoch: 92 | MAE Train Loss: 0.20688819885253906 | MAE Test Loss: 0.36925622820854187 \n",
      "Loss: 0.20573611557483673\n",
      "Epoch: 93 | MAE Train Loss: 0.20573611557483673 | MAE Test Loss: 0.36790916323661804 \n",
      "Loss: 0.2045840322971344\n",
      "Epoch: 94 | MAE Train Loss: 0.2045840322971344 | MAE Test Loss: 0.3665620684623718 \n",
      "Loss: 0.20343191921710968\n",
      "Epoch: 95 | MAE Train Loss: 0.20343191921710968 | MAE Test Loss: 0.36521491408348083 \n",
      "Loss: 0.20227983593940735\n",
      "Epoch: 96 | MAE Train Loss: 0.20227983593940735 | MAE Test Loss: 0.3638678789138794 \n",
      "Loss: 0.20112769305706024\n",
      "Epoch: 97 | MAE Train Loss: 0.20112769305706024 | MAE Test Loss: 0.3625207543373108 \n",
      "Loss: 0.1999756097793579\n",
      "Epoch: 98 | MAE Train Loss: 0.1999756097793579 | MAE Test Loss: 0.36117368936538696 \n",
      "Loss: 0.1988234966993332\n",
      "Epoch: 99 | MAE Train Loss: 0.1988234966993332 | MAE Test Loss: 0.35982653498649597 \n",
      "Loss: 0.19767141342163086\n",
      "Epoch: 100 | MAE Train Loss: 0.19767141342163086 | MAE Test Loss: 0.35847947001457214 \n",
      "Loss: 0.19651933014392853\n",
      "Epoch: 101 | MAE Train Loss: 0.19651933014392853 | MAE Test Loss: 0.3571323752403259 \n",
      "Loss: 0.19536720216274261\n",
      "Epoch: 102 | MAE Train Loss: 0.19536720216274261 | MAE Test Loss: 0.3557852506637573 \n",
      "Loss: 0.19421511888504028\n",
      "Epoch: 103 | MAE Train Loss: 0.19421511888504028 | MAE Test Loss: 0.3544381558895111 \n",
      "Loss: 0.19306302070617676\n",
      "Epoch: 104 | MAE Train Loss: 0.19306302070617676 | MAE Test Loss: 0.35309112071990967 \n",
      "Loss: 0.19191092252731323\n",
      "Epoch: 105 | MAE Train Loss: 0.19191092252731323 | MAE Test Loss: 0.35174399614334106 \n",
      "Loss: 0.19075879454612732\n",
      "Epoch: 106 | MAE Train Loss: 0.19075879454612732 | MAE Test Loss: 0.35039690136909485 \n",
      "Loss: 0.189606711268425\n",
      "Epoch: 107 | MAE Train Loss: 0.189606711268425 | MAE Test Loss: 0.34904980659484863 \n",
      "Loss: 0.18845462799072266\n",
      "Epoch: 108 | MAE Train Loss: 0.18845462799072266 | MAE Test Loss: 0.3477027118206024 \n",
      "Loss: 0.18730251491069794\n",
      "Epoch: 109 | MAE Train Loss: 0.18730251491069794 | MAE Test Loss: 0.3463556468486786 \n",
      "Loss: 0.18615040183067322\n",
      "Epoch: 110 | MAE Train Loss: 0.18615040183067322 | MAE Test Loss: 0.34500852227211 \n",
      "Loss: 0.18499833345413208\n",
      "Epoch: 111 | MAE Train Loss: 0.18499833345413208 | MAE Test Loss: 0.34366142749786377 \n",
      "Loss: 0.18384622037410736\n",
      "Epoch: 112 | MAE Train Loss: 0.18384622037410736 | MAE Test Loss: 0.34231433272361755 \n",
      "Loss: 0.18269412219524384\n",
      "Epoch: 113 | MAE Train Loss: 0.18269412219524384 | MAE Test Loss: 0.3409672677516937 \n",
      "Loss: 0.1815420240163803\n",
      "Epoch: 114 | MAE Train Loss: 0.1815420240163803 | MAE Test Loss: 0.3396201729774475 \n",
      "Loss: 0.1803899109363556\n",
      "Epoch: 115 | MAE Train Loss: 0.1803899109363556 | MAE Test Loss: 0.3382730484008789 \n",
      "Loss: 0.17923781275749207\n",
      "Epoch: 116 | MAE Train Loss: 0.17923781275749207 | MAE Test Loss: 0.3369259238243103 \n",
      "Loss: 0.17808572947978973\n",
      "Epoch: 117 | MAE Train Loss: 0.17808572947978973 | MAE Test Loss: 0.3355788290500641 \n",
      "Loss: 0.17693361639976501\n",
      "Epoch: 118 | MAE Train Loss: 0.17693361639976501 | MAE Test Loss: 0.33423173427581787 \n",
      "Loss: 0.1757815182209015\n",
      "Epoch: 119 | MAE Train Loss: 0.1757815182209015 | MAE Test Loss: 0.33288463950157166 \n",
      "Loss: 0.17462942004203796\n",
      "Epoch: 120 | MAE Train Loss: 0.17462942004203796 | MAE Test Loss: 0.33153754472732544 \n",
      "Loss: 0.17347732186317444\n",
      "Epoch: 121 | MAE Train Loss: 0.17347732186317444 | MAE Test Loss: 0.3301904797554016 \n",
      "Loss: 0.1723252385854721\n",
      "Epoch: 122 | MAE Train Loss: 0.1723252385854721 | MAE Test Loss: 0.3288434147834778 \n",
      "Loss: 0.17117314040660858\n",
      "Epoch: 123 | MAE Train Loss: 0.17117314040660858 | MAE Test Loss: 0.3274962902069092 \n",
      "Loss: 0.17002108693122864\n",
      "Epoch: 124 | MAE Train Loss: 0.17002108693122864 | MAE Test Loss: 0.3261492848396301 \n",
      "Loss: 0.1688689887523651\n",
      "Epoch: 125 | MAE Train Loss: 0.1688689887523651 | MAE Test Loss: 0.3248021602630615 \n",
      "Loss: 0.16771690547466278\n",
      "Epoch: 126 | MAE Train Loss: 0.16771690547466278 | MAE Test Loss: 0.3234550952911377 \n",
      "Loss: 0.16656479239463806\n",
      "Epoch: 127 | MAE Train Loss: 0.16656479239463806 | MAE Test Loss: 0.32210803031921387 \n",
      "Loss: 0.16541273891925812\n",
      "Epoch: 128 | MAE Train Loss: 0.16541273891925812 | MAE Test Loss: 0.32076090574264526 \n",
      "Loss: 0.1642606407403946\n",
      "Epoch: 129 | MAE Train Loss: 0.1642606407403946 | MAE Test Loss: 0.31941384077072144 \n",
      "Loss: 0.16310855746269226\n",
      "Epoch: 130 | MAE Train Loss: 0.16310855746269226 | MAE Test Loss: 0.3180667459964752 \n",
      "Loss: 0.16195647418498993\n",
      "Epoch: 131 | MAE Train Loss: 0.16195647418498993 | MAE Test Loss: 0.3167196810245514 \n",
      "Loss: 0.1608043909072876\n",
      "Epoch: 132 | MAE Train Loss: 0.1608043909072876 | MAE Test Loss: 0.3153725862503052 \n",
      "Loss: 0.15965232253074646\n",
      "Epoch: 133 | MAE Train Loss: 0.15965232253074646 | MAE Test Loss: 0.31402549147605896 \n",
      "Loss: 0.15850022435188293\n",
      "Epoch: 134 | MAE Train Loss: 0.15850022435188293 | MAE Test Loss: 0.31267839670181274 \n",
      "Loss: 0.1573481261730194\n",
      "Epoch: 135 | MAE Train Loss: 0.1573481261730194 | MAE Test Loss: 0.3113313317298889 \n",
      "Loss: 0.15619607269763947\n",
      "Epoch: 136 | MAE Train Loss: 0.15619607269763947 | MAE Test Loss: 0.3099842667579651 \n",
      "Loss: 0.15504398941993713\n",
      "Epoch: 137 | MAE Train Loss: 0.15504398941993713 | MAE Test Loss: 0.30863717198371887 \n",
      "Loss: 0.1538918912410736\n",
      "Epoch: 138 | MAE Train Loss: 0.1538918912410736 | MAE Test Loss: 0.30729013681411743 \n",
      "Loss: 0.15273980796337128\n",
      "Epoch: 139 | MAE Train Loss: 0.15273980796337128 | MAE Test Loss: 0.30594301223754883 \n",
      "Loss: 0.15158770978450775\n",
      "Epoch: 140 | MAE Train Loss: 0.15158770978450775 | MAE Test Loss: 0.304595947265625 \n",
      "Loss: 0.1504356414079666\n",
      "Epoch: 141 | MAE Train Loss: 0.1504356414079666 | MAE Test Loss: 0.30324888229370117 \n",
      "Loss: 0.14928355813026428\n",
      "Epoch: 142 | MAE Train Loss: 0.14928355813026428 | MAE Test Loss: 0.30190175771713257 \n",
      "Loss: 0.14813147485256195\n",
      "Epoch: 143 | MAE Train Loss: 0.14813147485256195 | MAE Test Loss: 0.30055469274520874 \n",
      "Loss: 0.14697937667369843\n",
      "Epoch: 144 | MAE Train Loss: 0.14697937667369843 | MAE Test Loss: 0.2992075979709625 \n",
      "Loss: 0.1458273082971573\n",
      "Epoch: 145 | MAE Train Loss: 0.1458273082971573 | MAE Test Loss: 0.2978605329990387 \n",
      "Loss: 0.14467521011829376\n",
      "Epoch: 146 | MAE Train Loss: 0.14467521011829376 | MAE Test Loss: 0.2965134382247925 \n",
      "Loss: 0.14352312684059143\n",
      "Epoch: 147 | MAE Train Loss: 0.14352312684059143 | MAE Test Loss: 0.29516634345054626 \n",
      "Loss: 0.1423710286617279\n",
      "Epoch: 148 | MAE Train Loss: 0.1423710286617279 | MAE Test Loss: 0.29381924867630005 \n",
      "Loss: 0.14121896028518677\n",
      "Epoch: 149 | MAE Train Loss: 0.14121896028518677 | MAE Test Loss: 0.292472243309021 \n",
      "Loss: 0.14006686210632324\n",
      "Epoch: 150 | MAE Train Loss: 0.14006686210632324 | MAE Test Loss: 0.2911251187324524 \n",
      "Loss: 0.1389147937297821\n",
      "Epoch: 151 | MAE Train Loss: 0.1389147937297821 | MAE Test Loss: 0.28977805376052856 \n",
      "Loss: 0.13776269555091858\n",
      "Epoch: 152 | MAE Train Loss: 0.13776269555091858 | MAE Test Loss: 0.28843095898628235 \n",
      "Loss: 0.13661061227321625\n",
      "Epoch: 153 | MAE Train Loss: 0.13661061227321625 | MAE Test Loss: 0.28708386421203613 \n",
      "Loss: 0.1354585438966751\n",
      "Epoch: 154 | MAE Train Loss: 0.1354585438966751 | MAE Test Loss: 0.2857367992401123 \n",
      "Loss: 0.13430646061897278\n",
      "Epoch: 155 | MAE Train Loss: 0.13430646061897278 | MAE Test Loss: 0.2843897342681885 \n",
      "Loss: 0.13315437734127045\n",
      "Epoch: 156 | MAE Train Loss: 0.13315437734127045 | MAE Test Loss: 0.28304266929626465 \n",
      "Loss: 0.13200227916240692\n",
      "Epoch: 157 | MAE Train Loss: 0.13200227916240692 | MAE Test Loss: 0.28169554471969604 \n",
      "Loss: 0.13085021078586578\n",
      "Epoch: 158 | MAE Train Loss: 0.13085021078586578 | MAE Test Loss: 0.2803484797477722 \n",
      "Loss: 0.12969812750816345\n",
      "Epoch: 159 | MAE Train Loss: 0.12969812750816345 | MAE Test Loss: 0.279001384973526 \n",
      "Loss: 0.12854602932929993\n",
      "Epoch: 160 | MAE Train Loss: 0.12854602932929993 | MAE Test Loss: 0.2776543200016022 \n",
      "Loss: 0.1273939311504364\n",
      "Epoch: 161 | MAE Train Loss: 0.1273939311504364 | MAE Test Loss: 0.27630725502967834 \n",
      "Loss: 0.12624187767505646\n",
      "Epoch: 162 | MAE Train Loss: 0.12624187767505646 | MAE Test Loss: 0.27496016025543213 \n",
      "Loss: 0.12508977949619293\n",
      "Epoch: 163 | MAE Train Loss: 0.12508977949619293 | MAE Test Loss: 0.2736130952835083 \n",
      "Loss: 0.12393768876791\n",
      "Epoch: 164 | MAE Train Loss: 0.12393768876791 | MAE Test Loss: 0.2722659707069397 \n",
      "Loss: 0.12278560549020767\n",
      "Epoch: 165 | MAE Train Loss: 0.12278560549020767 | MAE Test Loss: 0.27091890573501587 \n",
      "Loss: 0.12163352966308594\n",
      "Epoch: 166 | MAE Train Loss: 0.12163352966308594 | MAE Test Loss: 0.26957181096076965 \n",
      "Loss: 0.1204814463853836\n",
      "Epoch: 167 | MAE Train Loss: 0.1204814463853836 | MAE Test Loss: 0.2682247459888458 \n",
      "Loss: 0.11932934820652008\n",
      "Epoch: 168 | MAE Train Loss: 0.11932934820652008 | MAE Test Loss: 0.2668776512145996 \n",
      "Loss: 0.11817727237939835\n",
      "Epoch: 169 | MAE Train Loss: 0.11817727237939835 | MAE Test Loss: 0.2655305862426758 \n",
      "Loss: 0.11702518165111542\n",
      "Epoch: 170 | MAE Train Loss: 0.11702518165111542 | MAE Test Loss: 0.26418352127075195 \n",
      "Loss: 0.11587311327457428\n",
      "Epoch: 171 | MAE Train Loss: 0.11587311327457428 | MAE Test Loss: 0.26283639669418335 \n",
      "Loss: 0.11476147174835205\n",
      "Epoch: 172 | MAE Train Loss: 0.11476147174835205 | MAE Test Loss: 0.2615393400192261 \n",
      "Loss: 0.11370686441659927\n",
      "Epoch: 173 | MAE Train Loss: 0.11370686441659927 | MAE Test Loss: 0.260242223739624 \n",
      "Loss: 0.1126522570848465\n",
      "Epoch: 174 | MAE Train Loss: 0.1126522570848465 | MAE Test Loss: 0.258945107460022 \n",
      "Loss: 0.11159764230251312\n",
      "Epoch: 175 | MAE Train Loss: 0.11159764230251312 | MAE Test Loss: 0.2576480209827423 \n",
      "Loss: 0.11054304987192154\n",
      "Epoch: 176 | MAE Train Loss: 0.11054304987192154 | MAE Test Loss: 0.25635093450546265 \n",
      "Loss: 0.10948844254016876\n",
      "Epoch: 177 | MAE Train Loss: 0.10948844254016876 | MAE Test Loss: 0.2550538182258606 \n",
      "Loss: 0.10846539586782455\n",
      "Epoch: 178 | MAE Train Loss: 0.10846539586782455 | MAE Test Loss: 0.25380760431289673 \n",
      "Loss: 0.10750406980514526\n",
      "Epoch: 179 | MAE Train Loss: 0.10750406980514526 | MAE Test Loss: 0.2525613605976105 \n",
      "Loss: 0.10654275119304657\n",
      "Epoch: 180 | MAE Train Loss: 0.10654275119304657 | MAE Test Loss: 0.251315176486969 \n",
      "Loss: 0.10558142513036728\n",
      "Epoch: 181 | MAE Train Loss: 0.10558142513036728 | MAE Test Loss: 0.25006893277168274 \n",
      "Loss: 0.1046200841665268\n",
      "Epoch: 182 | MAE Train Loss: 0.1046200841665268 | MAE Test Loss: 0.24882273375988007 \n",
      "Loss: 0.1036587730050087\n",
      "Epoch: 183 | MAE Train Loss: 0.1036587730050087 | MAE Test Loss: 0.247576504945755 \n",
      "Loss: 0.10270978510379791\n",
      "Epoch: 184 | MAE Train Loss: 0.10270978510379791 | MAE Test Loss: 0.24638208746910095 \n",
      "Loss: 0.10183751583099365\n",
      "Epoch: 185 | MAE Train Loss: 0.10183751583099365 | MAE Test Loss: 0.2451876401901245 \n",
      "Loss: 0.10096526145935059\n",
      "Epoch: 186 | MAE Train Loss: 0.10096526145935059 | MAE Test Loss: 0.24399319291114807 \n",
      "Loss: 0.10009298473596573\n",
      "Epoch: 187 | MAE Train Loss: 0.10009298473596573 | MAE Test Loss: 0.24279877543449402 \n",
      "Loss: 0.09922071546316147\n",
      "Epoch: 188 | MAE Train Loss: 0.09922071546316147 | MAE Test Loss: 0.24160432815551758 \n",
      "Loss: 0.09834843873977661\n",
      "Epoch: 189 | MAE Train Loss: 0.09834843873977661 | MAE Test Loss: 0.24040989577770233 \n",
      "Loss: 0.09747617691755295\n",
      "Epoch: 190 | MAE Train Loss: 0.09747617691755295 | MAE Test Loss: 0.2392154484987259 \n",
      "Loss: 0.09663032740354538\n",
      "Epoch: 191 | MAE Train Loss: 0.09663032740354538 | MAE Test Loss: 0.2380737066268921 \n",
      "Loss: 0.09584285318851471\n",
      "Epoch: 192 | MAE Train Loss: 0.09584285318851471 | MAE Test Loss: 0.23693189024925232 \n",
      "Loss: 0.09505538642406464\n",
      "Epoch: 193 | MAE Train Loss: 0.09505538642406464 | MAE Test Loss: 0.23579013347625732 \n",
      "Loss: 0.09426790475845337\n",
      "Epoch: 194 | MAE Train Loss: 0.09426790475845337 | MAE Test Loss: 0.23464834690093994 \n",
      "Loss: 0.09348044544458389\n",
      "Epoch: 195 | MAE Train Loss: 0.09348044544458389 | MAE Test Loss: 0.23350660502910614 \n",
      "Loss: 0.09269297122955322\n",
      "Epoch: 196 | MAE Train Loss: 0.09269297122955322 | MAE Test Loss: 0.23236480355262756 \n",
      "Loss: 0.09190551191568375\n",
      "Epoch: 197 | MAE Train Loss: 0.09190551191568375 | MAE Test Loss: 0.23122303187847137 \n",
      "Loss: 0.09114636480808258\n",
      "Epoch: 198 | MAE Train Loss: 0.09114636480808258 | MAE Test Loss: 0.23013481497764587 \n",
      "Loss: 0.09043945372104645\n",
      "Epoch: 199 | MAE Train Loss: 0.09043945372104645 | MAE Test Loss: 0.22904661297798157 \n",
      "Loss: 0.08973254263401031\n",
      "Epoch: 200 | MAE Train Loss: 0.08973254263401031 | MAE Test Loss: 0.22795839607715607 \n",
      "Loss: 0.08902563154697418\n",
      "Epoch: 201 | MAE Train Loss: 0.08902563154697418 | MAE Test Loss: 0.22687020897865295 \n",
      "Loss: 0.08831872791051865\n",
      "Epoch: 202 | MAE Train Loss: 0.08831872791051865 | MAE Test Loss: 0.22578194737434387 \n",
      "Loss: 0.08761182427406311\n",
      "Epoch: 203 | MAE Train Loss: 0.08761182427406311 | MAE Test Loss: 0.22469374537467957 \n",
      "Loss: 0.08690490573644638\n",
      "Epoch: 204 | MAE Train Loss: 0.08690490573644638 | MAE Test Loss: 0.22360554337501526 \n",
      "Loss: 0.08621595799922943\n",
      "Epoch: 205 | MAE Train Loss: 0.08621595799922943 | MAE Test Loss: 0.22257176041603088 \n",
      "Loss: 0.0855853408575058\n",
      "Epoch: 206 | MAE Train Loss: 0.0855853408575058 | MAE Test Loss: 0.22153803706169128 \n",
      "Loss: 0.08495471626520157\n",
      "Epoch: 207 | MAE Train Loss: 0.08495471626520157 | MAE Test Loss: 0.2205042839050293 \n",
      "Loss: 0.08432409167289734\n",
      "Epoch: 208 | MAE Train Loss: 0.08432409167289734 | MAE Test Loss: 0.2194705307483673 \n",
      "Loss: 0.08369346708059311\n",
      "Epoch: 209 | MAE Train Loss: 0.08369346708059311 | MAE Test Loss: 0.21843676269054413 \n",
      "Loss: 0.08306284993886948\n",
      "Epoch: 210 | MAE Train Loss: 0.08306284993886948 | MAE Test Loss: 0.21740305423736572 \n",
      "Loss: 0.08243221789598465\n",
      "Epoch: 211 | MAE Train Loss: 0.08243221789598465 | MAE Test Loss: 0.21636930108070374 \n",
      "Loss: 0.08180160075426102\n",
      "Epoch: 212 | MAE Train Loss: 0.08180160075426102 | MAE Test Loss: 0.21533556282520294 \n",
      "Loss: 0.08120343089103699\n",
      "Epoch: 213 | MAE Train Loss: 0.08120343089103699 | MAE Test Loss: 0.21435710787773132 \n",
      "Loss: 0.08064477145671844\n",
      "Epoch: 214 | MAE Train Loss: 0.08064477145671844 | MAE Test Loss: 0.21337871253490448 \n",
      "Loss: 0.0800861045718193\n",
      "Epoch: 215 | MAE Train Loss: 0.0800861045718193 | MAE Test Loss: 0.21240031719207764 \n",
      "Loss: 0.07952745258808136\n",
      "Epoch: 216 | MAE Train Loss: 0.07952745258808136 | MAE Test Loss: 0.2114218920469284 \n",
      "Loss: 0.07896878570318222\n",
      "Epoch: 217 | MAE Train Loss: 0.07896878570318222 | MAE Test Loss: 0.21044349670410156 \n",
      "Loss: 0.07841013371944427\n",
      "Epoch: 218 | MAE Train Loss: 0.07841013371944427 | MAE Test Loss: 0.20946505665779114 \n",
      "Loss: 0.07785148918628693\n",
      "Epoch: 219 | MAE Train Loss: 0.07785148918628693 | MAE Test Loss: 0.2084866464138031 \n",
      "Loss: 0.07729282230138779\n",
      "Epoch: 220 | MAE Train Loss: 0.07729282230138779 | MAE Test Loss: 0.20750825107097626 \n",
      "Loss: 0.07676678895950317\n",
      "Epoch: 221 | MAE Train Loss: 0.07676678895950317 | MAE Test Loss: 0.206586092710495 \n",
      "Loss: 0.07627572864294052\n",
      "Epoch: 222 | MAE Train Loss: 0.07627572864294052 | MAE Test Loss: 0.20566387474536896 \n",
      "Loss: 0.07578467577695847\n",
      "Epoch: 223 | MAE Train Loss: 0.07578467577695847 | MAE Test Loss: 0.20474162697792053 \n",
      "Loss: 0.07529361546039581\n",
      "Epoch: 224 | MAE Train Loss: 0.07529361546039581 | MAE Test Loss: 0.20381946861743927 \n",
      "Loss: 0.07480257004499435\n",
      "Epoch: 225 | MAE Train Loss: 0.07480257004499435 | MAE Test Loss: 0.20289726555347443 \n",
      "Loss: 0.0743115097284317\n",
      "Epoch: 226 | MAE Train Loss: 0.0743115097284317 | MAE Test Loss: 0.20197506248950958 \n",
      "Loss: 0.07382046431303024\n",
      "Epoch: 227 | MAE Train Loss: 0.07382046431303024 | MAE Test Loss: 0.20105287432670593 \n",
      "Loss: 0.07332941144704819\n",
      "Epoch: 228 | MAE Train Loss: 0.07332941144704819 | MAE Test Loss: 0.2001306712627411 \n",
      "Loss: 0.072856605052948\n",
      "Epoch: 229 | MAE Train Loss: 0.072856605052948 | MAE Test Loss: 0.19926562905311584 \n",
      "Loss: 0.07242877781391144\n",
      "Epoch: 230 | MAE Train Loss: 0.07242877781391144 | MAE Test Loss: 0.19840054214000702 \n",
      "Loss: 0.07200097292661667\n",
      "Epoch: 231 | MAE Train Loss: 0.07200097292661667 | MAE Test Loss: 0.19753551483154297 \n",
      "Loss: 0.0715731531381607\n",
      "Epoch: 232 | MAE Train Loss: 0.0715731531381607 | MAE Test Loss: 0.19667045772075653 \n",
      "Loss: 0.07114534080028534\n",
      "Epoch: 233 | MAE Train Loss: 0.07114534080028534 | MAE Test Loss: 0.19580543041229248 \n",
      "Loss: 0.07071752846240997\n",
      "Epoch: 234 | MAE Train Loss: 0.07071752846240997 | MAE Test Loss: 0.19494035840034485 \n",
      "Loss: 0.0702897235751152\n",
      "Epoch: 235 | MAE Train Loss: 0.0702897235751152 | MAE Test Loss: 0.19407527148723602 \n",
      "Loss: 0.06986190378665924\n",
      "Epoch: 236 | MAE Train Loss: 0.06986190378665924 | MAE Test Loss: 0.19321021437644958 \n",
      "Loss: 0.06943409144878387\n",
      "Epoch: 237 | MAE Train Loss: 0.06943409144878387 | MAE Test Loss: 0.19234518706798553 \n",
      "Loss: 0.06902603805065155\n",
      "Epoch: 238 | MAE Train Loss: 0.06902603805065155 | MAE Test Loss: 0.19153812527656555 \n",
      "Loss: 0.06865701824426651\n",
      "Epoch: 239 | MAE Train Loss: 0.06865701824426651 | MAE Test Loss: 0.19073110818862915 \n",
      "Loss: 0.06828799843788147\n",
      "Epoch: 240 | MAE Train Loss: 0.06828799843788147 | MAE Test Loss: 0.18992407619953156 \n",
      "Loss: 0.06791897863149643\n",
      "Epoch: 241 | MAE Train Loss: 0.06791897863149643 | MAE Test Loss: 0.18911704421043396 \n",
      "Loss: 0.06754995882511139\n",
      "Epoch: 242 | MAE Train Loss: 0.06754995882511139 | MAE Test Loss: 0.18830999732017517 \n",
      "Loss: 0.06718094646930695\n",
      "Epoch: 243 | MAE Train Loss: 0.06718094646930695 | MAE Test Loss: 0.18750298023223877 \n",
      "Loss: 0.0668119341135025\n",
      "Epoch: 244 | MAE Train Loss: 0.0668119341135025 | MAE Test Loss: 0.18669594824314117 \n",
      "Loss: 0.06644289940595627\n",
      "Epoch: 245 | MAE Train Loss: 0.06644289940595627 | MAE Test Loss: 0.18588891625404358 \n",
      "Loss: 0.06607388705015182\n",
      "Epoch: 246 | MAE Train Loss: 0.06607388705015182 | MAE Test Loss: 0.1850818693637848 \n",
      "Loss: 0.06570921093225479\n",
      "Epoch: 247 | MAE Train Loss: 0.06570921093225479 | MAE Test Loss: 0.18433372676372528 \n",
      "Loss: 0.06539449840784073\n",
      "Epoch: 248 | MAE Train Loss: 0.06539449840784073 | MAE Test Loss: 0.18358558416366577 \n",
      "Loss: 0.06507977098226547\n",
      "Epoch: 249 | MAE Train Loss: 0.06507977098226547 | MAE Test Loss: 0.18283741176128387 \n",
      "Loss: 0.06476505100727081\n",
      "Epoch: 250 | MAE Train Loss: 0.06476505100727081 | MAE Test Loss: 0.18208928406238556 \n",
      "Loss: 0.06445033103227615\n",
      "Epoch: 251 | MAE Train Loss: 0.06445033103227615 | MAE Test Loss: 0.18134114146232605 \n",
      "Loss: 0.0641356036067009\n",
      "Epoch: 252 | MAE Train Loss: 0.0641356036067009 | MAE Test Loss: 0.18059298396110535 \n",
      "Loss: 0.06382088363170624\n",
      "Epoch: 253 | MAE Train Loss: 0.06382088363170624 | MAE Test Loss: 0.17984485626220703 \n",
      "Loss: 0.06350617110729218\n",
      "Epoch: 254 | MAE Train Loss: 0.06350617110729218 | MAE Test Loss: 0.17909672856330872 \n",
      "Loss: 0.06319145113229752\n",
      "Epoch: 255 | MAE Train Loss: 0.06319145113229752 | MAE Test Loss: 0.178348571062088 \n",
      "Loss: 0.06287671625614166\n",
      "Epoch: 256 | MAE Train Loss: 0.06287671625614166 | MAE Test Loss: 0.1776004284620285 \n",
      "Loss: 0.0625620037317276\n",
      "Epoch: 257 | MAE Train Loss: 0.0625620037317276 | MAE Test Loss: 0.1768522709608078 \n",
      "Loss: 0.062271296977996826\n",
      "Epoch: 258 | MAE Train Loss: 0.062271296977996826 | MAE Test Loss: 0.1761639267206192 \n",
      "Loss: 0.06200631335377693\n",
      "Epoch: 259 | MAE Train Loss: 0.06200631335377693 | MAE Test Loss: 0.17547550797462463 \n",
      "Loss: 0.06174134090542793\n",
      "Epoch: 260 | MAE Train Loss: 0.06174134090542793 | MAE Test Loss: 0.17478716373443604 \n",
      "Loss: 0.06147634983062744\n",
      "Epoch: 261 | MAE Train Loss: 0.06147634983062744 | MAE Test Loss: 0.17409880459308624 \n",
      "Loss: 0.061211369931697845\n",
      "Epoch: 262 | MAE Train Loss: 0.061211369931697845 | MAE Test Loss: 0.17341041564941406 \n",
      "Loss: 0.06094638258218765\n",
      "Epoch: 263 | MAE Train Loss: 0.06094638258218765 | MAE Test Loss: 0.17272204160690308 \n",
      "Loss: 0.060681410133838654\n",
      "Epoch: 264 | MAE Train Loss: 0.060681410133838654 | MAE Test Loss: 0.1720336377620697 \n",
      "Loss: 0.06041641905903816\n",
      "Epoch: 265 | MAE Train Loss: 0.06041641905903816 | MAE Test Loss: 0.1713452786207199 \n",
      "Loss: 0.060151439160108566\n",
      "Epoch: 266 | MAE Train Loss: 0.060151439160108566 | MAE Test Loss: 0.17065691947937012 \n",
      "Loss: 0.05988645553588867\n",
      "Epoch: 267 | MAE Train Loss: 0.05988645553588867 | MAE Test Loss: 0.16996853053569794 \n",
      "Loss: 0.05962147191166878\n",
      "Epoch: 268 | MAE Train Loss: 0.05962147191166878 | MAE Test Loss: 0.16928015649318695 \n",
      "Loss: 0.05937860533595085\n",
      "Epoch: 269 | MAE Train Loss: 0.05937860533595085 | MAE Test Loss: 0.1686524599790573 \n",
      "Loss: 0.05915876105427742\n",
      "Epoch: 270 | MAE Train Loss: 0.05915876105427742 | MAE Test Loss: 0.16802480816841125 \n",
      "Loss: 0.05893892049789429\n",
      "Epoch: 271 | MAE Train Loss: 0.05893892049789429 | MAE Test Loss: 0.16739711165428162 \n",
      "Loss: 0.05871908739209175\n",
      "Epoch: 272 | MAE Train Loss: 0.05871908739209175 | MAE Test Loss: 0.16676943004131317 \n",
      "Loss: 0.05849923565983772\n",
      "Epoch: 273 | MAE Train Loss: 0.05849923565983772 | MAE Test Loss: 0.16614174842834473 \n",
      "Loss: 0.05827939510345459\n",
      "Epoch: 274 | MAE Train Loss: 0.05827939510345459 | MAE Test Loss: 0.16551408171653748 \n",
      "Loss: 0.05805954337120056\n",
      "Epoch: 275 | MAE Train Loss: 0.05805954337120056 | MAE Test Loss: 0.16488640010356903 \n",
      "Loss: 0.05783969908952713\n",
      "Epoch: 276 | MAE Train Loss: 0.05783969908952713 | MAE Test Loss: 0.16425874829292297 \n",
      "Loss: 0.057619858533144\n",
      "Epoch: 277 | MAE Train Loss: 0.057619858533144 | MAE Test Loss: 0.16363103687763214 \n",
      "Loss: 0.05740001052618027\n",
      "Epoch: 278 | MAE Train Loss: 0.05740001052618027 | MAE Test Loss: 0.1630033701658249 \n",
      "Loss: 0.057180166244506836\n",
      "Epoch: 279 | MAE Train Loss: 0.057180166244506836 | MAE Test Loss: 0.16237568855285645 \n",
      "Loss: 0.0569603256881237\n",
      "Epoch: 280 | MAE Train Loss: 0.0569603256881237 | MAE Test Loss: 0.161748006939888 \n",
      "Loss: 0.05676015466451645\n",
      "Epoch: 281 | MAE Train Loss: 0.05676015466451645 | MAE Test Loss: 0.16118189692497253 \n",
      "Loss: 0.05658075958490372\n",
      "Epoch: 282 | MAE Train Loss: 0.05658075958490372 | MAE Test Loss: 0.16061577200889587 \n",
      "Loss: 0.05640135332942009\n",
      "Epoch: 283 | MAE Train Loss: 0.05640135332942009 | MAE Test Loss: 0.1600496768951416 \n",
      "Loss: 0.05622195079922676\n",
      "Epoch: 284 | MAE Train Loss: 0.05622195079922676 | MAE Test Loss: 0.15948358178138733 \n",
      "Loss: 0.05604255199432373\n",
      "Epoch: 285 | MAE Train Loss: 0.05604255199432373 | MAE Test Loss: 0.15891747176647186 \n",
      "Loss: 0.0558631531894207\n",
      "Epoch: 286 | MAE Train Loss: 0.0558631531894207 | MAE Test Loss: 0.1583513766527176 \n",
      "Loss: 0.05568375438451767\n",
      "Epoch: 287 | MAE Train Loss: 0.05568375438451767 | MAE Test Loss: 0.15778525173664093 \n",
      "Loss: 0.05550435930490494\n",
      "Epoch: 288 | MAE Train Loss: 0.05550435930490494 | MAE Test Loss: 0.15721914172172546 \n",
      "Loss: 0.05532495304942131\n",
      "Epoch: 289 | MAE Train Loss: 0.05532495304942131 | MAE Test Loss: 0.1566530466079712 \n",
      "Loss: 0.05514555424451828\n",
      "Epoch: 290 | MAE Train Loss: 0.05514555424451828 | MAE Test Loss: 0.15608695149421692 \n",
      "Loss: 0.054966144263744354\n",
      "Epoch: 291 | MAE Train Loss: 0.054966144263744354 | MAE Test Loss: 0.15552084147930145 \n",
      "Loss: 0.05478675290942192\n",
      "Epoch: 292 | MAE Train Loss: 0.05478675290942192 | MAE Test Loss: 0.1549547165632248 \n",
      "Loss: 0.05460735037922859\n",
      "Epoch: 293 | MAE Train Loss: 0.05460735037922859 | MAE Test Loss: 0.15438862144947052 \n",
      "Loss: 0.05443967133760452\n",
      "Epoch: 294 | MAE Train Loss: 0.05443967133760452 | MAE Test Loss: 0.15388496220111847 \n",
      "Loss: 0.05429593846201897\n",
      "Epoch: 295 | MAE Train Loss: 0.05429593846201897 | MAE Test Loss: 0.15338130295276642 \n",
      "Loss: 0.05415221303701401\n",
      "Epoch: 296 | MAE Train Loss: 0.05415221303701401 | MAE Test Loss: 0.15287765860557556 \n",
      "Loss: 0.05400847643613815\n",
      "Epoch: 297 | MAE Train Loss: 0.05400847643613815 | MAE Test Loss: 0.15237398445606232 \n",
      "Loss: 0.05386475846171379\n",
      "Epoch: 298 | MAE Train Loss: 0.05386475846171379 | MAE Test Loss: 0.15187029540538788 \n",
      "Loss: 0.05372103303670883\n",
      "Epoch: 299 | MAE Train Loss: 0.05372103303670883 | MAE Test Loss: 0.15136666595935822 \n",
      "Loss: 0.053577303886413574\n",
      "Epoch: 300 | MAE Train Loss: 0.053577303886413574 | MAE Test Loss: 0.15086300671100616 \n",
      "Loss: 0.05343357473611832\n",
      "Epoch: 301 | MAE Train Loss: 0.05343357473611832 | MAE Test Loss: 0.1503593474626541 \n",
      "Loss: 0.05328984186053276\n",
      "Epoch: 302 | MAE Train Loss: 0.05328984186053276 | MAE Test Loss: 0.14985567331314087 \n",
      "Loss: 0.0531461238861084\n",
      "Epoch: 303 | MAE Train Loss: 0.0531461238861084 | MAE Test Loss: 0.14935199916362762 \n",
      "Loss: 0.05300239473581314\n",
      "Epoch: 304 | MAE Train Loss: 0.05300239473581314 | MAE Test Loss: 0.14884835481643677 \n",
      "Loss: 0.052858661860227585\n",
      "Epoch: 305 | MAE Train Loss: 0.052858661860227585 | MAE Test Loss: 0.14834468066692352 \n",
      "Loss: 0.052714936435222626\n",
      "Epoch: 306 | MAE Train Loss: 0.052714936435222626 | MAE Test Loss: 0.14784102141857147 \n",
      "Loss: 0.05257120728492737\n",
      "Epoch: 307 | MAE Train Loss: 0.05257120728492737 | MAE Test Loss: 0.1473373919725418 \n",
      "Loss: 0.05242748185992241\n",
      "Epoch: 308 | MAE Train Loss: 0.05242748185992241 | MAE Test Loss: 0.14683370292186737 \n",
      "Loss: 0.05229362100362778\n",
      "Epoch: 309 | MAE Train Loss: 0.05229362100362778 | MAE Test Loss: 0.14639338850975037 \n",
      "Loss: 0.05218071490526199\n",
      "Epoch: 310 | MAE Train Loss: 0.05218071490526199 | MAE Test Loss: 0.14595307409763336 \n",
      "Loss: 0.05206780880689621\n",
      "Epoch: 311 | MAE Train Loss: 0.05206780880689621 | MAE Test Loss: 0.14551277458667755 \n",
      "Loss: 0.05195491388440132\n",
      "Epoch: 312 | MAE Train Loss: 0.05195491388440132 | MAE Test Loss: 0.14507248997688293 \n",
      "Loss: 0.05184202268719673\n",
      "Epoch: 313 | MAE Train Loss: 0.05184202268719673 | MAE Test Loss: 0.14463214576244354 \n",
      "Loss: 0.05172910541296005\n",
      "Epoch: 314 | MAE Train Loss: 0.05172910541296005 | MAE Test Loss: 0.14419183135032654 \n",
      "Loss: 0.05161619931459427\n",
      "Epoch: 315 | MAE Train Loss: 0.05161619931459427 | MAE Test Loss: 0.14375153183937073 \n",
      "Loss: 0.05150330066680908\n",
      "Epoch: 316 | MAE Train Loss: 0.05150330066680908 | MAE Test Loss: 0.14331121742725372 \n",
      "Loss: 0.0513903982937336\n",
      "Epoch: 317 | MAE Train Loss: 0.0513903982937336 | MAE Test Loss: 0.14287090301513672 \n",
      "Loss: 0.05127749592065811\n",
      "Epoch: 318 | MAE Train Loss: 0.05127749592065811 | MAE Test Loss: 0.14243058860301971 \n",
      "Loss: 0.05116458982229233\n",
      "Epoch: 319 | MAE Train Loss: 0.05116458982229233 | MAE Test Loss: 0.1419902741909027 \n",
      "Loss: 0.05105169489979744\n",
      "Epoch: 320 | MAE Train Loss: 0.05105169489979744 | MAE Test Loss: 0.1415499746799469 \n",
      "Loss: 0.05093878507614136\n",
      "Epoch: 321 | MAE Train Loss: 0.05093878507614136 | MAE Test Loss: 0.1411096602678299 \n",
      "Loss: 0.050825875252485275\n",
      "Epoch: 322 | MAE Train Loss: 0.050825875252485275 | MAE Test Loss: 0.1406693160533905 \n",
      "Loss: 0.05071298032999039\n",
      "Epoch: 323 | MAE Train Loss: 0.05071298032999039 | MAE Test Loss: 0.1402290314435959 \n",
      "Loss: 0.0506000742316246\n",
      "Epoch: 324 | MAE Train Loss: 0.0506000742316246 | MAE Test Loss: 0.13978871703147888 \n",
      "Loss: 0.050487179309129715\n",
      "Epoch: 325 | MAE Train Loss: 0.050487179309129715 | MAE Test Loss: 0.13934841752052307 \n",
      "Loss: 0.050378382205963135\n",
      "Epoch: 326 | MAE Train Loss: 0.050378382205963135 | MAE Test Loss: 0.13897234201431274 \n",
      "Loss: 0.050291359424591064\n",
      "Epoch: 327 | MAE Train Loss: 0.050291359424591064 | MAE Test Loss: 0.1385962963104248 \n",
      "Loss: 0.050204355269670486\n",
      "Epoch: 328 | MAE Train Loss: 0.050204355269670486 | MAE Test Loss: 0.13822023570537567 \n",
      "Loss: 0.050117332488298416\n",
      "Epoch: 329 | MAE Train Loss: 0.050117332488298416 | MAE Test Loss: 0.13784417510032654 \n",
      "Loss: 0.050030313432216644\n",
      "Epoch: 330 | MAE Train Loss: 0.050030313432216644 | MAE Test Loss: 0.1374681293964386 \n",
      "Loss: 0.049943309277296066\n",
      "Epoch: 331 | MAE Train Loss: 0.049943309277296066 | MAE Test Loss: 0.13709203898906708 \n",
      "Loss: 0.0498562753200531\n",
      "Epoch: 332 | MAE Train Loss: 0.0498562753200531 | MAE Test Loss: 0.13671600818634033 \n",
      "Loss: 0.049769263714551926\n",
      "Epoch: 333 | MAE Train Loss: 0.049769263714551926 | MAE Test Loss: 0.13633993268013 \n",
      "Loss: 0.04968224838376045\n",
      "Epoch: 334 | MAE Train Loss: 0.04968224838376045 | MAE Test Loss: 0.13596387207508087 \n",
      "Loss: 0.04959522932767868\n",
      "Epoch: 335 | MAE Train Loss: 0.04959522932767868 | MAE Test Loss: 0.13558778166770935 \n",
      "Loss: 0.049508217722177505\n",
      "Epoch: 336 | MAE Train Loss: 0.049508217722177505 | MAE Test Loss: 0.1352117508649826 \n",
      "Loss: 0.049421198666095734\n",
      "Epoch: 337 | MAE Train Loss: 0.049421198666095734 | MAE Test Loss: 0.13483569025993347 \n",
      "Loss: 0.04933418333530426\n",
      "Epoch: 338 | MAE Train Loss: 0.04933418333530426 | MAE Test Loss: 0.13445964455604553 \n",
      "Loss: 0.04924716800451279\n",
      "Epoch: 339 | MAE Train Loss: 0.04924716800451279 | MAE Test Loss: 0.1340835690498352 \n",
      "Loss: 0.049160148948431015\n",
      "Epoch: 340 | MAE Train Loss: 0.049160148948431015 | MAE Test Loss: 0.13370750844478607 \n",
      "Loss: 0.04907312989234924\n",
      "Epoch: 341 | MAE Train Loss: 0.04907312989234924 | MAE Test Loss: 0.13333146274089813 \n",
      "Loss: 0.04898611456155777\n",
      "Epoch: 342 | MAE Train Loss: 0.04898611456155777 | MAE Test Loss: 0.1329553872346878 \n",
      "Loss: 0.048899102956056595\n",
      "Epoch: 343 | MAE Train Loss: 0.048899102956056595 | MAE Test Loss: 0.13257935643196106 \n",
      "Loss: 0.04881208389997482\n",
      "Epoch: 344 | MAE Train Loss: 0.04881208389997482 | MAE Test Loss: 0.13220326602458954 \n",
      "Loss: 0.04872506856918335\n",
      "Epoch: 345 | MAE Train Loss: 0.04872506856918335 | MAE Test Loss: 0.1318272203207016 \n",
      "Loss: 0.048638053238391876\n",
      "Epoch: 346 | MAE Train Loss: 0.048638053238391876 | MAE Test Loss: 0.13145115971565247 \n",
      "Loss: 0.04855869710445404\n",
      "Epoch: 347 | MAE Train Loss: 0.04855869710445404 | MAE Test Loss: 0.13114026188850403 \n",
      "Loss: 0.048492539674043655\n",
      "Epoch: 348 | MAE Train Loss: 0.048492539674043655 | MAE Test Loss: 0.1308293491601944 \n",
      "Loss: 0.04842637851834297\n",
      "Epoch: 349 | MAE Train Loss: 0.04842637851834297 | MAE Test Loss: 0.13051843643188477 \n",
      "Loss: 0.04836020618677139\n",
      "Epoch: 350 | MAE Train Loss: 0.04836020618677139 | MAE Test Loss: 0.13020753860473633 \n",
      "Loss: 0.04829404503107071\n",
      "Epoch: 351 | MAE Train Loss: 0.04829404503107071 | MAE Test Loss: 0.1298966407775879 \n",
      "Loss: 0.04822787269949913\n",
      "Epoch: 352 | MAE Train Loss: 0.04822787269949913 | MAE Test Loss: 0.12958571314811707 \n",
      "Loss: 0.048161715269088745\n",
      "Epoch: 353 | MAE Train Loss: 0.048161715269088745 | MAE Test Loss: 0.12927480041980743 \n",
      "Loss: 0.048095546662807465\n",
      "Epoch: 354 | MAE Train Loss: 0.048095546662807465 | MAE Test Loss: 0.128963902592659 \n",
      "Loss: 0.04802938178181648\n",
      "Epoch: 355 | MAE Train Loss: 0.04802938178181648 | MAE Test Loss: 0.12865300476551056 \n",
      "Loss: 0.0479632243514061\n",
      "Epoch: 356 | MAE Train Loss: 0.0479632243514061 | MAE Test Loss: 0.12834210693836212 \n",
      "Loss: 0.04789704829454422\n",
      "Epoch: 357 | MAE Train Loss: 0.04789704829454422 | MAE Test Loss: 0.1280311793088913 \n",
      "Loss: 0.047830890864133835\n",
      "Epoch: 358 | MAE Train Loss: 0.047830890864133835 | MAE Test Loss: 0.12772028148174286 \n",
      "Loss: 0.04776472598314285\n",
      "Epoch: 359 | MAE Train Loss: 0.04776472598314285 | MAE Test Loss: 0.12740938365459442 \n",
      "Loss: 0.04769856110215187\n",
      "Epoch: 360 | MAE Train Loss: 0.04769856110215187 | MAE Test Loss: 0.12709848582744598 \n",
      "Loss: 0.04763239622116089\n",
      "Epoch: 361 | MAE Train Loss: 0.04763239622116089 | MAE Test Loss: 0.12678757309913635 \n",
      "Loss: 0.04756622761487961\n",
      "Epoch: 362 | MAE Train Loss: 0.04756622761487961 | MAE Test Loss: 0.12647667527198792 \n",
      "Loss: 0.04750007390975952\n",
      "Epoch: 363 | MAE Train Loss: 0.04750007390975952 | MAE Test Loss: 0.12616576254367828 \n",
      "Loss: 0.04743390530347824\n",
      "Epoch: 364 | MAE Train Loss: 0.04743390530347824 | MAE Test Loss: 0.12585484981536865 \n",
      "Loss: 0.04736773669719696\n",
      "Epoch: 365 | MAE Train Loss: 0.04736773669719696 | MAE Test Loss: 0.12554392218589783 \n",
      "Loss: 0.04730156809091568\n",
      "Epoch: 366 | MAE Train Loss: 0.04730156809091568 | MAE Test Loss: 0.1252330243587494 \n",
      "Loss: 0.0472353920340538\n",
      "Epoch: 367 | MAE Train Loss: 0.0472353920340538 | MAE Test Loss: 0.12492211908102036 \n",
      "Loss: 0.047169238328933716\n",
      "Epoch: 368 | MAE Train Loss: 0.047169238328933716 | MAE Test Loss: 0.12461123615503311 \n",
      "Loss: 0.047103073447942734\n",
      "Epoch: 369 | MAE Train Loss: 0.047103073447942734 | MAE Test Loss: 0.12430031597614288 \n",
      "Loss: 0.04703690484166145\n",
      "Epoch: 370 | MAE Train Loss: 0.04703690484166145 | MAE Test Loss: 0.12398938834667206 \n",
      "Loss: 0.04697074741125107\n",
      "Epoch: 371 | MAE Train Loss: 0.04697074741125107 | MAE Test Loss: 0.12367852032184601 \n",
      "Loss: 0.04690460115671158\n",
      "Epoch: 372 | MAE Train Loss: 0.04690460115671158 | MAE Test Loss: 0.12343358993530273 \n",
      "Loss: 0.04685414209961891\n",
      "Epoch: 373 | MAE Train Loss: 0.04685414209961891 | MAE Test Loss: 0.12318868935108185 \n",
      "Loss: 0.046803683042526245\n",
      "Epoch: 374 | MAE Train Loss: 0.046803683042526245 | MAE Test Loss: 0.12294378131628036 \n",
      "Loss: 0.04675322771072388\n",
      "Epoch: 375 | MAE Train Loss: 0.04675322771072388 | MAE Test Loss: 0.12269886583089828 \n",
      "Loss: 0.04670276492834091\n",
      "Epoch: 376 | MAE Train Loss: 0.04670276492834091 | MAE Test Loss: 0.122453972697258 \n",
      "Loss: 0.046652305871248245\n",
      "Epoch: 377 | MAE Train Loss: 0.046652305871248245 | MAE Test Loss: 0.12220907211303711 \n",
      "Loss: 0.04660185053944588\n",
      "Epoch: 378 | MAE Train Loss: 0.04660185053944588 | MAE Test Loss: 0.12196417152881622 \n",
      "Loss: 0.04655139893293381\n",
      "Epoch: 379 | MAE Train Loss: 0.04655139893293381 | MAE Test Loss: 0.12171925604343414 \n",
      "Loss: 0.046500932425260544\n",
      "Epoch: 380 | MAE Train Loss: 0.046500932425260544 | MAE Test Loss: 0.12147434055805206 \n",
      "Loss: 0.04645047336816788\n",
      "Epoch: 381 | MAE Train Loss: 0.04645047336816788 | MAE Test Loss: 0.12122943252325058 \n",
      "Loss: 0.04640001803636551\n",
      "Epoch: 382 | MAE Train Loss: 0.04640001803636551 | MAE Test Loss: 0.1209845319390297 \n",
      "Loss: 0.04634954780340195\n",
      "Epoch: 383 | MAE Train Loss: 0.04634954780340195 | MAE Test Loss: 0.12073962390422821 \n",
      "Loss: 0.04629909247159958\n",
      "Epoch: 384 | MAE Train Loss: 0.04629909247159958 | MAE Test Loss: 0.12049472332000732 \n",
      "Loss: 0.04624864086508751\n",
      "Epoch: 385 | MAE Train Loss: 0.04624864086508751 | MAE Test Loss: 0.12024979293346405 \n",
      "Loss: 0.046198178082704544\n",
      "Epoch: 386 | MAE Train Loss: 0.046198178082704544 | MAE Test Loss: 0.12000487744808197 \n",
      "Loss: 0.046147722750902176\n",
      "Epoch: 387 | MAE Train Loss: 0.046147722750902176 | MAE Test Loss: 0.11975999176502228 \n",
      "Loss: 0.04609726741909981\n",
      "Epoch: 388 | MAE Train Loss: 0.04609726741909981 | MAE Test Loss: 0.1195150837302208 \n",
      "Loss: 0.04604680463671684\n",
      "Epoch: 389 | MAE Train Loss: 0.04604680463671684 | MAE Test Loss: 0.11927016079425812 \n",
      "Loss: 0.045996345579624176\n",
      "Epoch: 390 | MAE Train Loss: 0.045996345579624176 | MAE Test Loss: 0.11902527511119843 \n",
      "Loss: 0.04594588279724121\n",
      "Epoch: 391 | MAE Train Loss: 0.04594588279724121 | MAE Test Loss: 0.11878037452697754 \n",
      "Loss: 0.045895423740148544\n",
      "Epoch: 392 | MAE Train Loss: 0.045895423740148544 | MAE Test Loss: 0.11853544414043427 \n",
      "Loss: 0.04584496468305588\n",
      "Epoch: 393 | MAE Train Loss: 0.04584496468305588 | MAE Test Loss: 0.11829052865505219 \n",
      "Loss: 0.045794516801834106\n",
      "Epoch: 394 | MAE Train Loss: 0.045794516801834106 | MAE Test Loss: 0.1180456131696701 \n",
      "Loss: 0.04574405029416084\n",
      "Epoch: 395 | MAE Train Loss: 0.04574405029416084 | MAE Test Loss: 0.11780073493719101 \n",
      "Loss: 0.04569358751177788\n",
      "Epoch: 396 | MAE Train Loss: 0.04569358751177788 | MAE Test Loss: 0.11755583435297012 \n",
      "Loss: 0.04564313963055611\n",
      "Epoch: 397 | MAE Train Loss: 0.04564313963055611 | MAE Test Loss: 0.11731092631816864 \n",
      "Loss: 0.04559267684817314\n",
      "Epoch: 398 | MAE Train Loss: 0.04559267684817314 | MAE Test Loss: 0.11706600338220596 \n",
      "Loss: 0.04554221034049988\n",
      "Epoch: 399 | MAE Train Loss: 0.04554221034049988 | MAE Test Loss: 0.11682109534740448 \n",
      "Loss: 0.04549176245927811\n",
      "Epoch: 400 | MAE Train Loss: 0.04549176245927811 | MAE Test Loss: 0.11657620966434479 \n",
      "Loss: 0.04544129967689514\n",
      "Epoch: 401 | MAE Train Loss: 0.04544129967689514 | MAE Test Loss: 0.11633126437664032 \n",
      "Loss: 0.04539085179567337\n",
      "Epoch: 402 | MAE Train Loss: 0.04539085179567337 | MAE Test Loss: 0.11608636379241943 \n",
      "Loss: 0.04534038156270981\n",
      "Epoch: 403 | MAE Train Loss: 0.04534038156270981 | MAE Test Loss: 0.11584146320819855 \n",
      "Loss: 0.04528992623090744\n",
      "Epoch: 404 | MAE Train Loss: 0.04528992623090744 | MAE Test Loss: 0.11559655517339706 \n",
      "Loss: 0.04523947089910507\n",
      "Epoch: 405 | MAE Train Loss: 0.04523947089910507 | MAE Test Loss: 0.11535165458917618 \n",
      "Loss: 0.045189011842012405\n",
      "Epoch: 406 | MAE Train Loss: 0.045189011842012405 | MAE Test Loss: 0.1151067465543747 \n",
      "Loss: 0.04513854533433914\n",
      "Epoch: 407 | MAE Train Loss: 0.04513854533433914 | MAE Test Loss: 0.1148618683218956 \n",
      "Loss: 0.04509454965591431\n",
      "Epoch: 408 | MAE Train Loss: 0.04509454965591431 | MAE Test Loss: 0.1146838441491127 \n",
      "Loss: 0.045054562389850616\n",
      "Epoch: 409 | MAE Train Loss: 0.045054562389850616 | MAE Test Loss: 0.1145058423280716 \n",
      "Loss: 0.045014552772045135\n",
      "Epoch: 410 | MAE Train Loss: 0.045014552772045135 | MAE Test Loss: 0.11432783305644989 \n",
      "Loss: 0.04497454687952995\n",
      "Epoch: 411 | MAE Train Loss: 0.04497454687952995 | MAE Test Loss: 0.11414983123540878 \n",
      "Loss: 0.04493454843759537\n",
      "Epoch: 412 | MAE Train Loss: 0.04493454843759537 | MAE Test Loss: 0.11397182941436768 \n",
      "Loss: 0.04489455372095108\n",
      "Epoch: 413 | MAE Train Loss: 0.04489455372095108 | MAE Test Loss: 0.11379382759332657 \n",
      "Loss: 0.0448545478284359\n",
      "Epoch: 414 | MAE Train Loss: 0.0448545478284359 | MAE Test Loss: 0.11361582577228546 \n",
      "Loss: 0.044814541935920715\n",
      "Epoch: 415 | MAE Train Loss: 0.044814541935920715 | MAE Test Loss: 0.11343781650066376 \n",
      "Loss: 0.04477455094456673\n",
      "Epoch: 416 | MAE Train Loss: 0.04477455094456673 | MAE Test Loss: 0.11325981467962265 \n",
      "Loss: 0.044734545052051544\n",
      "Epoch: 417 | MAE Train Loss: 0.044734545052051544 | MAE Test Loss: 0.11308183521032333 \n",
      "Loss: 0.04469454661011696\n",
      "Epoch: 418 | MAE Train Loss: 0.04469454661011696 | MAE Test Loss: 0.11290381103754044 \n",
      "Loss: 0.044654544442892075\n",
      "Epoch: 419 | MAE Train Loss: 0.044654544442892075 | MAE Test Loss: 0.11272580921649933 \n",
      "Loss: 0.04461454600095749\n",
      "Epoch: 420 | MAE Train Loss: 0.04461454600095749 | MAE Test Loss: 0.11254779994487762 \n",
      "Loss: 0.0445745475590229\n",
      "Epoch: 421 | MAE Train Loss: 0.0445745475590229 | MAE Test Loss: 0.11236979812383652 \n",
      "Loss: 0.04453454539179802\n",
      "Epoch: 422 | MAE Train Loss: 0.04453454539179802 | MAE Test Loss: 0.1121918112039566 \n",
      "Loss: 0.044494546949863434\n",
      "Epoch: 423 | MAE Train Loss: 0.044494546949863434 | MAE Test Loss: 0.1120137944817543 \n",
      "Loss: 0.04445454478263855\n",
      "Epoch: 424 | MAE Train Loss: 0.04445454478263855 | MAE Test Loss: 0.1118357926607132 \n",
      "Loss: 0.044414542615413666\n",
      "Epoch: 425 | MAE Train Loss: 0.044414542615413666 | MAE Test Loss: 0.11165778338909149 \n",
      "Loss: 0.04437453672289848\n",
      "Epoch: 426 | MAE Train Loss: 0.04437453672289848 | MAE Test Loss: 0.11147978156805038 \n",
      "Loss: 0.0443345345556736\n",
      "Epoch: 427 | MAE Train Loss: 0.0443345345556736 | MAE Test Loss: 0.11130179464817047 \n",
      "Loss: 0.044294536113739014\n",
      "Epoch: 428 | MAE Train Loss: 0.044294536113739014 | MAE Test Loss: 0.11112377792596817 \n",
      "Loss: 0.04425454139709473\n",
      "Epoch: 429 | MAE Train Loss: 0.04425454139709473 | MAE Test Loss: 0.11094577610492706 \n",
      "Loss: 0.04421453922986984\n",
      "Epoch: 430 | MAE Train Loss: 0.04421453922986984 | MAE Test Loss: 0.11076776683330536 \n",
      "Loss: 0.04417453333735466\n",
      "Epoch: 431 | MAE Train Loss: 0.04417453333735466 | MAE Test Loss: 0.11058976501226425 \n",
      "Loss: 0.04413453862071037\n",
      "Epoch: 432 | MAE Train Loss: 0.04413453862071037 | MAE Test Loss: 0.11041177809238434 \n",
      "Loss: 0.04409453272819519\n",
      "Epoch: 433 | MAE Train Loss: 0.04409453272819519 | MAE Test Loss: 0.11023376137018204 \n",
      "Loss: 0.044054530560970306\n",
      "Epoch: 434 | MAE Train Loss: 0.044054530560970306 | MAE Test Loss: 0.11005575954914093 \n",
      "Loss: 0.04401453956961632\n",
      "Epoch: 435 | MAE Train Loss: 0.04401453956961632 | MAE Test Loss: 0.10987772792577744 \n",
      "Loss: 0.04397452995181084\n",
      "Epoch: 436 | MAE Train Loss: 0.04397452995181084 | MAE Test Loss: 0.10969976335763931 \n",
      "Loss: 0.04393453150987625\n",
      "Epoch: 437 | MAE Train Loss: 0.04393453150987625 | MAE Test Loss: 0.1095217615365982 \n",
      "Loss: 0.043894533067941666\n",
      "Epoch: 438 | MAE Train Loss: 0.043894533067941666 | MAE Test Loss: 0.1093437448143959 \n",
      "Loss: 0.04385452717542648\n",
      "Epoch: 439 | MAE Train Loss: 0.04385452717542648 | MAE Test Loss: 0.1091657504439354 \n",
      "Loss: 0.043814532458782196\n",
      "Epoch: 440 | MAE Train Loss: 0.043814532458782196 | MAE Test Loss: 0.1089877337217331 \n",
      "Loss: 0.043774526566267014\n",
      "Epoch: 441 | MAE Train Loss: 0.043774526566267014 | MAE Test Loss: 0.10880974680185318 \n",
      "Loss: 0.043734531849622726\n",
      "Epoch: 442 | MAE Train Loss: 0.043734531849622726 | MAE Test Loss: 0.10863174498081207 \n",
      "Loss: 0.04369453340768814\n",
      "Epoch: 443 | MAE Train Loss: 0.04369453340768814 | MAE Test Loss: 0.10845372825860977 \n",
      "Loss: 0.04365452378988266\n",
      "Epoch: 444 | MAE Train Loss: 0.04365452378988266 | MAE Test Loss: 0.10827572643756866 \n",
      "Loss: 0.04361452907323837\n",
      "Epoch: 445 | MAE Train Loss: 0.04361452907323837 | MAE Test Loss: 0.10809771716594696 \n",
      "Loss: 0.04357453063130379\n",
      "Epoch: 446 | MAE Train Loss: 0.04357453063130379 | MAE Test Loss: 0.10791973024606705 \n",
      "Loss: 0.043534524738788605\n",
      "Epoch: 447 | MAE Train Loss: 0.043534524738788605 | MAE Test Loss: 0.10774173587560654 \n",
      "Loss: 0.04349452629685402\n",
      "Epoch: 448 | MAE Train Loss: 0.04349452629685402 | MAE Test Loss: 0.10756371915340424 \n",
      "Loss: 0.043454527854919434\n",
      "Epoch: 449 | MAE Train Loss: 0.043454527854919434 | MAE Test Loss: 0.10738573223352432 \n",
      "Loss: 0.04341452196240425\n",
      "Epoch: 450 | MAE Train Loss: 0.04341452196240425 | MAE Test Loss: 0.10720770061016083 \n",
      "Loss: 0.04337451979517937\n",
      "Epoch: 451 | MAE Train Loss: 0.04337451979517937 | MAE Test Loss: 0.10702969878911972 \n",
      "Loss: 0.04333452507853508\n",
      "Epoch: 452 | MAE Train Loss: 0.04333452507853508 | MAE Test Loss: 0.1068517193198204 \n",
      "Loss: 0.043294526636600494\n",
      "Epoch: 453 | MAE Train Loss: 0.043294526636600494 | MAE Test Loss: 0.1066737174987793 \n",
      "Loss: 0.04325452074408531\n",
      "Epoch: 454 | MAE Train Loss: 0.04325452074408531 | MAE Test Loss: 0.106495700776577 \n",
      "Loss: 0.04321451857686043\n",
      "Epoch: 455 | MAE Train Loss: 0.04321451857686043 | MAE Test Loss: 0.1063176840543747 \n",
      "Loss: 0.04317452758550644\n",
      "Epoch: 456 | MAE Train Loss: 0.04317452758550644 | MAE Test Loss: 0.10613970458507538 \n",
      "Loss: 0.04313451796770096\n",
      "Epoch: 457 | MAE Train Loss: 0.04313451796770096 | MAE Test Loss: 0.10596170276403427 \n",
      "Loss: 0.04309451952576637\n",
      "Epoch: 458 | MAE Train Loss: 0.04309451952576637 | MAE Test Loss: 0.10578368604183197 \n",
      "Loss: 0.04305451363325119\n",
      "Epoch: 459 | MAE Train Loss: 0.04305451363325119 | MAE Test Loss: 0.10560569912195206 \n",
      "Loss: 0.04301450774073601\n",
      "Epoch: 460 | MAE Train Loss: 0.04301450774073601 | MAE Test Loss: 0.10542766749858856 \n",
      "Loss: 0.04297452047467232\n",
      "Epoch: 461 | MAE Train Loss: 0.04297452047467232 | MAE Test Loss: 0.10524968802928925 \n",
      "Loss: 0.042934514582157135\n",
      "Epoch: 462 | MAE Train Loss: 0.042934514582157135 | MAE Test Loss: 0.10507168620824814 \n",
      "Loss: 0.04289551451802254\n",
      "Epoch: 463 | MAE Train Loss: 0.04289551451802254 | MAE Test Loss: 0.10496147722005844 \n",
      "Loss: 0.04286061227321625\n",
      "Epoch: 464 | MAE Train Loss: 0.04286061227321625 | MAE Test Loss: 0.10485129058361053 \n",
      "Loss: 0.04282570630311966\n",
      "Epoch: 465 | MAE Train Loss: 0.04282570630311966 | MAE Test Loss: 0.10474108159542084 \n",
      "Loss: 0.04279080778360367\n",
      "Epoch: 466 | MAE Train Loss: 0.04279080778360367 | MAE Test Loss: 0.10463090240955353 \n",
      "Loss: 0.04275590553879738\n",
      "Epoch: 467 | MAE Train Loss: 0.04275590553879738 | MAE Test Loss: 0.10452067852020264 \n",
      "Loss: 0.042720992118120193\n",
      "Epoch: 468 | MAE Train Loss: 0.042720992118120193 | MAE Test Loss: 0.10441050678491592 \n",
      "Loss: 0.0426861010491848\n",
      "Epoch: 469 | MAE Train Loss: 0.0426861010491848 | MAE Test Loss: 0.10430029779672623 \n",
      "Loss: 0.04265119880437851\n",
      "Epoch: 470 | MAE Train Loss: 0.04265119880437851 | MAE Test Loss: 0.10419009625911713 \n",
      "Loss: 0.04261629655957222\n",
      "Epoch: 471 | MAE Train Loss: 0.04261629655957222 | MAE Test Loss: 0.10407988727092743 \n",
      "Loss: 0.04258139804005623\n",
      "Epoch: 472 | MAE Train Loss: 0.04258139804005623 | MAE Test Loss: 0.10396971553564072 \n",
      "Loss: 0.04254649952054024\n",
      "Epoch: 473 | MAE Train Loss: 0.04254649952054024 | MAE Test Loss: 0.10385950654745102 \n",
      "Loss: 0.04251159727573395\n",
      "Epoch: 474 | MAE Train Loss: 0.04251159727573395 | MAE Test Loss: 0.10374931991100311 \n",
      "Loss: 0.04247669130563736\n",
      "Epoch: 475 | MAE Train Loss: 0.04247669130563736 | MAE Test Loss: 0.10363911092281342 \n",
      "Loss: 0.04244178533554077\n",
      "Epoch: 476 | MAE Train Loss: 0.04244178533554077 | MAE Test Loss: 0.1035289317369461 \n",
      "Loss: 0.04240688681602478\n",
      "Epoch: 477 | MAE Train Loss: 0.04240688681602478 | MAE Test Loss: 0.10341870784759521 \n",
      "Loss: 0.042371977120637894\n",
      "Epoch: 478 | MAE Train Loss: 0.042371977120637894 | MAE Test Loss: 0.1033085361123085 \n",
      "Loss: 0.0423370823264122\n",
      "Epoch: 479 | MAE Train Loss: 0.0423370823264122 | MAE Test Loss: 0.1031983271241188 \n",
      "Loss: 0.04230218380689621\n",
      "Epoch: 480 | MAE Train Loss: 0.04230218380689621 | MAE Test Loss: 0.1030881255865097 \n",
      "Loss: 0.04226727783679962\n",
      "Epoch: 481 | MAE Train Loss: 0.04226727783679962 | MAE Test Loss: 0.10297791659832001 \n",
      "Loss: 0.04223238304257393\n",
      "Epoch: 482 | MAE Train Loss: 0.04223238304257393 | MAE Test Loss: 0.1028677448630333 \n",
      "Loss: 0.04219748452305794\n",
      "Epoch: 483 | MAE Train Loss: 0.04219748452305794 | MAE Test Loss: 0.1027575358748436 \n",
      "Loss: 0.04216258227825165\n",
      "Epoch: 484 | MAE Train Loss: 0.04216258227825165 | MAE Test Loss: 0.10264734923839569 \n",
      "Loss: 0.04212767630815506\n",
      "Epoch: 485 | MAE Train Loss: 0.04212767630815506 | MAE Test Loss: 0.102537140250206 \n",
      "Loss: 0.04209277778863907\n",
      "Epoch: 486 | MAE Train Loss: 0.04209277778863907 | MAE Test Loss: 0.10242696106433868 \n",
      "Loss: 0.04205787554383278\n",
      "Epoch: 487 | MAE Train Loss: 0.04205787554383278 | MAE Test Loss: 0.10231673717498779 \n",
      "Loss: 0.042022962123155594\n",
      "Epoch: 488 | MAE Train Loss: 0.042022962123155594 | MAE Test Loss: 0.10220656543970108 \n",
      "Loss: 0.0419880710542202\n",
      "Epoch: 489 | MAE Train Loss: 0.0419880710542202 | MAE Test Loss: 0.10209635645151138 \n",
      "Loss: 0.04195316880941391\n",
      "Epoch: 490 | MAE Train Loss: 0.04195316880941391 | MAE Test Loss: 0.10198615491390228 \n",
      "Loss: 0.04191826656460762\n",
      "Epoch: 491 | MAE Train Loss: 0.04191826656460762 | MAE Test Loss: 0.10187594592571259 \n",
      "Loss: 0.04188336804509163\n",
      "Epoch: 492 | MAE Train Loss: 0.04188336804509163 | MAE Test Loss: 0.10176577419042587 \n",
      "Loss: 0.04184846952557564\n",
      "Epoch: 493 | MAE Train Loss: 0.04184846952557564 | MAE Test Loss: 0.10165556520223618 \n",
      "Loss: 0.04181356728076935\n",
      "Epoch: 494 | MAE Train Loss: 0.04181356728076935 | MAE Test Loss: 0.10154537856578827 \n",
      "Loss: 0.04177866131067276\n",
      "Epoch: 495 | MAE Train Loss: 0.04177866131067276 | MAE Test Loss: 0.10143516957759857 \n",
      "Loss: 0.04174375534057617\n",
      "Epoch: 496 | MAE Train Loss: 0.04174375534057617 | MAE Test Loss: 0.10132499039173126 \n",
      "Loss: 0.04170885682106018\n",
      "Epoch: 497 | MAE Train Loss: 0.04170885682106018 | MAE Test Loss: 0.10121476650238037 \n",
      "Loss: 0.041673947125673294\n",
      "Epoch: 498 | MAE Train Loss: 0.041673947125673294 | MAE Test Loss: 0.10110459476709366 \n",
      "Loss: 0.0416390523314476\n",
      "Epoch: 499 | MAE Train Loss: 0.0416390523314476 | MAE Test Loss: 0.10099438577890396 \n",
      "Loss: 0.04160415381193161\n",
      "Epoch: 500 | MAE Train Loss: 0.04160415381193161 | MAE Test Loss: 0.10088418424129486 \n",
      "Loss: 0.04156924784183502\n",
      "Epoch: 501 | MAE Train Loss: 0.04156924784183502 | MAE Test Loss: 0.10077397525310516 \n",
      "Loss: 0.04153435304760933\n",
      "Epoch: 502 | MAE Train Loss: 0.04153435304760933 | MAE Test Loss: 0.10066380351781845 \n",
      "Loss: 0.04149945452809334\n",
      "Epoch: 503 | MAE Train Loss: 0.04149945452809334 | MAE Test Loss: 0.10055359452962875 \n",
      "Loss: 0.04146455228328705\n",
      "Epoch: 504 | MAE Train Loss: 0.04146455228328705 | MAE Test Loss: 0.10044340789318085 \n",
      "Loss: 0.04142964631319046\n",
      "Epoch: 505 | MAE Train Loss: 0.04142964631319046 | MAE Test Loss: 0.10033319890499115 \n",
      "Loss: 0.04139474779367447\n",
      "Epoch: 506 | MAE Train Loss: 0.04139474779367447 | MAE Test Loss: 0.10022301971912384 \n",
      "Loss: 0.04135984554886818\n",
      "Epoch: 507 | MAE Train Loss: 0.04135984554886818 | MAE Test Loss: 0.10011279582977295 \n",
      "Loss: 0.041324932128190994\n",
      "Epoch: 508 | MAE Train Loss: 0.041324932128190994 | MAE Test Loss: 0.10000262409448624 \n",
      "Loss: 0.0412900410592556\n",
      "Epoch: 509 | MAE Train Loss: 0.0412900410592556 | MAE Test Loss: 0.09989241510629654 \n",
      "Loss: 0.04125513881444931\n",
      "Epoch: 510 | MAE Train Loss: 0.04125513881444931 | MAE Test Loss: 0.09978221356868744 \n",
      "Loss: 0.04122023656964302\n",
      "Epoch: 511 | MAE Train Loss: 0.04122023656964302 | MAE Test Loss: 0.09967201203107834 \n",
      "Loss: 0.04118533805012703\n",
      "Epoch: 512 | MAE Train Loss: 0.04118533805012703 | MAE Test Loss: 0.09956184029579163 \n",
      "Loss: 0.04115043953061104\n",
      "Epoch: 513 | MAE Train Loss: 0.04115043953061104 | MAE Test Loss: 0.09945162385702133 \n",
      "Loss: 0.04111553728580475\n",
      "Epoch: 514 | MAE Train Loss: 0.04111553728580475 | MAE Test Loss: 0.09934143722057343 \n",
      "Loss: 0.04108063131570816\n",
      "Epoch: 515 | MAE Train Loss: 0.04108063131570816 | MAE Test Loss: 0.09923122823238373 \n",
      "Loss: 0.04104572534561157\n",
      "Epoch: 516 | MAE Train Loss: 0.04104572534561157 | MAE Test Loss: 0.09912104904651642 \n",
      "Loss: 0.04101082682609558\n",
      "Epoch: 517 | MAE Train Loss: 0.04101082682609558 | MAE Test Loss: 0.09901082515716553 \n",
      "Loss: 0.040975917130708694\n",
      "Epoch: 518 | MAE Train Loss: 0.040975917130708694 | MAE Test Loss: 0.09890064597129822 \n",
      "Loss: 0.040941022336483\n",
      "Epoch: 519 | MAE Train Loss: 0.040941022336483 | MAE Test Loss: 0.09879044443368912 \n",
      "Loss: 0.04090612381696701\n",
      "Epoch: 520 | MAE Train Loss: 0.04090612381696701 | MAE Test Loss: 0.09868024289608002 \n",
      "Loss: 0.04087121784687042\n",
      "Epoch: 521 | MAE Train Loss: 0.04087121784687042 | MAE Test Loss: 0.09857004135847092 \n",
      "Loss: 0.04083632305264473\n",
      "Epoch: 522 | MAE Train Loss: 0.04083632305264473 | MAE Test Loss: 0.0984598696231842 \n",
      "Loss: 0.04080142453312874\n",
      "Epoch: 523 | MAE Train Loss: 0.04080142453312874 | MAE Test Loss: 0.09834965318441391 \n",
      "Loss: 0.04076652228832245\n",
      "Epoch: 524 | MAE Train Loss: 0.04076652228832245 | MAE Test Loss: 0.098239466547966 \n",
      "Loss: 0.04073161631822586\n",
      "Epoch: 525 | MAE Train Loss: 0.04073161631822586 | MAE Test Loss: 0.0981292575597763 \n",
      "Loss: 0.04069671779870987\n",
      "Epoch: 526 | MAE Train Loss: 0.04069671779870987 | MAE Test Loss: 0.098019078373909 \n",
      "Loss: 0.04066181555390358\n",
      "Epoch: 527 | MAE Train Loss: 0.04066181555390358 | MAE Test Loss: 0.0979088544845581 \n",
      "Loss: 0.040626902133226395\n",
      "Epoch: 528 | MAE Train Loss: 0.040626902133226395 | MAE Test Loss: 0.0977986752986908 \n",
      "Loss: 0.040592011064291\n",
      "Epoch: 529 | MAE Train Loss: 0.040592011064291 | MAE Test Loss: 0.0976884737610817 \n",
      "Loss: 0.04055710881948471\n",
      "Epoch: 530 | MAE Train Loss: 0.04055710881948471 | MAE Test Loss: 0.0975782722234726 \n",
      "Loss: 0.04052220657467842\n",
      "Epoch: 531 | MAE Train Loss: 0.04052220657467842 | MAE Test Loss: 0.0974680706858635 \n",
      "Loss: 0.04048730805516243\n",
      "Epoch: 532 | MAE Train Loss: 0.04048730805516243 | MAE Test Loss: 0.09735789895057678 \n",
      "Loss: 0.04045240953564644\n",
      "Epoch: 533 | MAE Train Loss: 0.04045240953564644 | MAE Test Loss: 0.09724768251180649 \n",
      "Loss: 0.04041750729084015\n",
      "Epoch: 534 | MAE Train Loss: 0.04041750729084015 | MAE Test Loss: 0.09713749587535858 \n",
      "Loss: 0.04038260132074356\n",
      "Epoch: 535 | MAE Train Loss: 0.04038260132074356 | MAE Test Loss: 0.09702728688716888 \n",
      "Loss: 0.04034769535064697\n",
      "Epoch: 536 | MAE Train Loss: 0.04034769535064697 | MAE Test Loss: 0.09691710770130157 \n",
      "Loss: 0.04031279683113098\n",
      "Epoch: 537 | MAE Train Loss: 0.04031279683113098 | MAE Test Loss: 0.09680688381195068 \n",
      "Loss: 0.040277887135744095\n",
      "Epoch: 538 | MAE Train Loss: 0.040277887135744095 | MAE Test Loss: 0.09669670462608337 \n",
      "Loss: 0.0402429923415184\n",
      "Epoch: 539 | MAE Train Loss: 0.0402429923415184 | MAE Test Loss: 0.09658650308847427 \n",
      "Loss: 0.04020809382200241\n",
      "Epoch: 540 | MAE Train Loss: 0.04020809382200241 | MAE Test Loss: 0.09647630155086517 \n",
      "Loss: 0.04017318785190582\n",
      "Epoch: 541 | MAE Train Loss: 0.04017318785190582 | MAE Test Loss: 0.09636610001325607 \n",
      "Loss: 0.04013829305768013\n",
      "Epoch: 542 | MAE Train Loss: 0.04013829305768013 | MAE Test Loss: 0.09625592827796936 \n",
      "Loss: 0.04010339453816414\n",
      "Epoch: 543 | MAE Train Loss: 0.04010339453816414 | MAE Test Loss: 0.09614574164152145 \n",
      "Loss: 0.04006849601864815\n",
      "Epoch: 544 | MAE Train Loss: 0.04006849601864815 | MAE Test Loss: 0.09603555500507355 \n",
      "Loss: 0.04003359004855156\n",
      "Epoch: 545 | MAE Train Loss: 0.04003359004855156 | MAE Test Loss: 0.09592534601688385 \n",
      "Loss: 0.03999868780374527\n",
      "Epoch: 546 | MAE Train Loss: 0.03999868780374527 | MAE Test Loss: 0.09581514447927475 \n",
      "Loss: 0.03996378555893898\n",
      "Epoch: 547 | MAE Train Loss: 0.03996378555893898 | MAE Test Loss: 0.09570496529340744 \n",
      "Loss: 0.03992888331413269\n",
      "Epoch: 548 | MAE Train Loss: 0.03992888331413269 | MAE Test Loss: 0.09559475630521774 \n",
      "Loss: 0.0398939847946167\n",
      "Epoch: 549 | MAE Train Loss: 0.0398939847946167 | MAE Test Loss: 0.09548455476760864 \n",
      "Loss: 0.03985908254981041\n",
      "Epoch: 550 | MAE Train Loss: 0.03985908254981041 | MAE Test Loss: 0.09537436813116074 \n",
      "Loss: 0.03982418030500412\n",
      "Epoch: 551 | MAE Train Loss: 0.03982418030500412 | MAE Test Loss: 0.09526417404413223 \n",
      "Loss: 0.03978928178548813\n",
      "Epoch: 552 | MAE Train Loss: 0.03978928178548813 | MAE Test Loss: 0.09515395760536194 \n",
      "Loss: 0.039754386991262436\n",
      "Epoch: 553 | MAE Train Loss: 0.039754386991262436 | MAE Test Loss: 0.09504377096891403 \n",
      "Loss: 0.03971948102116585\n",
      "Epoch: 554 | MAE Train Loss: 0.03971948102116585 | MAE Test Loss: 0.09493358433246613 \n",
      "Loss: 0.03968457505106926\n",
      "Epoch: 555 | MAE Train Loss: 0.03968457505106926 | MAE Test Loss: 0.09482337534427643 \n",
      "Loss: 0.03964967280626297\n",
      "Epoch: 556 | MAE Train Loss: 0.03964967280626297 | MAE Test Loss: 0.09471317380666733 \n",
      "Loss: 0.03961477428674698\n",
      "Epoch: 557 | MAE Train Loss: 0.03961477428674698 | MAE Test Loss: 0.09460299462080002 \n",
      "Loss: 0.03957986459136009\n",
      "Epoch: 558 | MAE Train Loss: 0.03957986459136009 | MAE Test Loss: 0.09449278563261032 \n",
      "Loss: 0.0395449697971344\n",
      "Epoch: 559 | MAE Train Loss: 0.0395449697971344 | MAE Test Loss: 0.09438258409500122 \n",
      "Loss: 0.03951007127761841\n",
      "Epoch: 560 | MAE Train Loss: 0.03951007127761841 | MAE Test Loss: 0.09427239745855331 \n",
      "Loss: 0.03947516530752182\n",
      "Epoch: 561 | MAE Train Loss: 0.03947516530752182 | MAE Test Loss: 0.09416220337152481 \n",
      "Loss: 0.03944026678800583\n",
      "Epoch: 562 | MAE Train Loss: 0.03944026678800583 | MAE Test Loss: 0.09405198693275452 \n",
      "Loss: 0.03940536826848984\n",
      "Epoch: 563 | MAE Train Loss: 0.03940536826848984 | MAE Test Loss: 0.09394180029630661 \n",
      "Loss: 0.03937046602368355\n",
      "Epoch: 564 | MAE Train Loss: 0.03937046602368355 | MAE Test Loss: 0.0938316136598587 \n",
      "Loss: 0.03933556005358696\n",
      "Epoch: 565 | MAE Train Loss: 0.03933556005358696 | MAE Test Loss: 0.093721404671669 \n",
      "Loss: 0.03930065780878067\n",
      "Epoch: 566 | MAE Train Loss: 0.03930065780878067 | MAE Test Loss: 0.0936112031340599 \n",
      "Loss: 0.03926575556397438\n",
      "Epoch: 567 | MAE Train Loss: 0.03926575556397438 | MAE Test Loss: 0.0935010239481926 \n",
      "Loss: 0.03923085331916809\n",
      "Epoch: 568 | MAE Train Loss: 0.03923085331916809 | MAE Test Loss: 0.0933908149600029 \n",
      "Loss: 0.0391959547996521\n",
      "Epoch: 569 | MAE Train Loss: 0.0391959547996521 | MAE Test Loss: 0.0932806134223938 \n",
      "Loss: 0.03916105255484581\n",
      "Epoch: 570 | MAE Train Loss: 0.03916105255484581 | MAE Test Loss: 0.09317042678594589 \n",
      "Loss: 0.03912615031003952\n",
      "Epoch: 571 | MAE Train Loss: 0.03912615031003952 | MAE Test Loss: 0.09306023269891739 \n",
      "Loss: 0.03909125179052353\n",
      "Epoch: 572 | MAE Train Loss: 0.03909125179052353 | MAE Test Loss: 0.0929500162601471 \n",
      "Loss: 0.039056356996297836\n",
      "Epoch: 573 | MAE Train Loss: 0.039056356996297836 | MAE Test Loss: 0.09283982962369919 \n",
      "Loss: 0.03902145102620125\n",
      "Epoch: 574 | MAE Train Loss: 0.03902145102620125 | MAE Test Loss: 0.09272964298725128 \n",
      "Loss: 0.03898654505610466\n",
      "Epoch: 575 | MAE Train Loss: 0.03898654505610466 | MAE Test Loss: 0.09261943399906158 \n",
      "Loss: 0.03895164281129837\n",
      "Epoch: 576 | MAE Train Loss: 0.03895164281129837 | MAE Test Loss: 0.09250923246145248 \n",
      "Loss: 0.03891674429178238\n",
      "Epoch: 577 | MAE Train Loss: 0.03891674429178238 | MAE Test Loss: 0.09239905327558517 \n",
      "Loss: 0.03888183459639549\n",
      "Epoch: 578 | MAE Train Loss: 0.03888183459639549 | MAE Test Loss: 0.09228884428739548 \n",
      "Loss: 0.0388469398021698\n",
      "Epoch: 579 | MAE Train Loss: 0.0388469398021698 | MAE Test Loss: 0.09217864274978638 \n",
      "Loss: 0.03881204128265381\n",
      "Epoch: 580 | MAE Train Loss: 0.03881204128265381 | MAE Test Loss: 0.09206845611333847 \n",
      "Loss: 0.03877713531255722\n",
      "Epoch: 581 | MAE Train Loss: 0.03877713531255722 | MAE Test Loss: 0.09195826202630997 \n",
      "Loss: 0.03874223679304123\n",
      "Epoch: 582 | MAE Train Loss: 0.03874223679304123 | MAE Test Loss: 0.09184804558753967 \n",
      "Loss: 0.03870733827352524\n",
      "Epoch: 583 | MAE Train Loss: 0.03870733827352524 | MAE Test Loss: 0.09173785895109177 \n",
      "Loss: 0.03867243602871895\n",
      "Epoch: 584 | MAE Train Loss: 0.03867243602871895 | MAE Test Loss: 0.09162767231464386 \n",
      "Loss: 0.03863753005862236\n",
      "Epoch: 585 | MAE Train Loss: 0.03863753005862236 | MAE Test Loss: 0.09151746332645416 \n",
      "Loss: 0.03860262781381607\n",
      "Epoch: 586 | MAE Train Loss: 0.03860262781381607 | MAE Test Loss: 0.09140726178884506 \n",
      "Loss: 0.03856772556900978\n",
      "Epoch: 587 | MAE Train Loss: 0.03856772556900978 | MAE Test Loss: 0.09129708260297775 \n",
      "Loss: 0.03853282332420349\n",
      "Epoch: 588 | MAE Train Loss: 0.03853282332420349 | MAE Test Loss: 0.09118687361478806 \n",
      "Loss: 0.0384979248046875\n",
      "Epoch: 589 | MAE Train Loss: 0.0384979248046875 | MAE Test Loss: 0.09107667207717896 \n",
      "Loss: 0.03846302255988121\n",
      "Epoch: 590 | MAE Train Loss: 0.03846302255988121 | MAE Test Loss: 0.09096648544073105 \n",
      "Loss: 0.03842812031507492\n",
      "Epoch: 591 | MAE Train Loss: 0.03842812031507492 | MAE Test Loss: 0.09085629135370255 \n",
      "Loss: 0.03839322179555893\n",
      "Epoch: 592 | MAE Train Loss: 0.03839322179555893 | MAE Test Loss: 0.09074607491493225 \n",
      "Loss: 0.03835832700133324\n",
      "Epoch: 593 | MAE Train Loss: 0.03835832700133324 | MAE Test Loss: 0.09063588827848434 \n",
      "Loss: 0.03832342103123665\n",
      "Epoch: 594 | MAE Train Loss: 0.03832342103123665 | MAE Test Loss: 0.09052570164203644 \n",
      "Loss: 0.03828851506114006\n",
      "Epoch: 595 | MAE Train Loss: 0.03828851506114006 | MAE Test Loss: 0.09041549265384674 \n",
      "Loss: 0.03825361281633377\n",
      "Epoch: 596 | MAE Train Loss: 0.03825361281633377 | MAE Test Loss: 0.09030529111623764 \n",
      "Loss: 0.03821871429681778\n",
      "Epoch: 597 | MAE Train Loss: 0.03821871429681778 | MAE Test Loss: 0.09019511193037033 \n",
      "Loss: 0.03818380460143089\n",
      "Epoch: 598 | MAE Train Loss: 0.03818380460143089 | MAE Test Loss: 0.09008490294218063 \n",
      "Loss: 0.0381489098072052\n",
      "Epoch: 599 | MAE Train Loss: 0.0381489098072052 | MAE Test Loss: 0.08997470140457153 \n",
      "Loss: 0.03811401128768921\n",
      "Epoch: 600 | MAE Train Loss: 0.03811401128768921 | MAE Test Loss: 0.08986451476812363 \n",
      "Loss: 0.03807910531759262\n",
      "Epoch: 601 | MAE Train Loss: 0.03807910531759262 | MAE Test Loss: 0.08975431323051453 \n",
      "Loss: 0.03804420679807663\n",
      "Epoch: 602 | MAE Train Loss: 0.03804420679807663 | MAE Test Loss: 0.08964410424232483 \n",
      "Loss: 0.03800930827856064\n",
      "Epoch: 603 | MAE Train Loss: 0.03800930827856064 | MAE Test Loss: 0.08953391760587692 \n",
      "Loss: 0.03797440603375435\n",
      "Epoch: 604 | MAE Train Loss: 0.03797440603375435 | MAE Test Loss: 0.08942373096942902 \n",
      "Loss: 0.03793950006365776\n",
      "Epoch: 605 | MAE Train Loss: 0.03793950006365776 | MAE Test Loss: 0.08931352198123932 \n",
      "Loss: 0.03790459781885147\n",
      "Epoch: 606 | MAE Train Loss: 0.03790459781885147 | MAE Test Loss: 0.08920332044363022 \n",
      "Loss: 0.03786969557404518\n",
      "Epoch: 607 | MAE Train Loss: 0.03786969557404518 | MAE Test Loss: 0.08909314125776291 \n",
      "Loss: 0.03783479332923889\n",
      "Epoch: 608 | MAE Train Loss: 0.03783479332923889 | MAE Test Loss: 0.08898293226957321 \n",
      "Loss: 0.0377998948097229\n",
      "Epoch: 609 | MAE Train Loss: 0.0377998948097229 | MAE Test Loss: 0.08887273073196411 \n",
      "Loss: 0.03776499256491661\n",
      "Epoch: 610 | MAE Train Loss: 0.03776499256491661 | MAE Test Loss: 0.0887625440955162 \n",
      "Loss: 0.03773009032011032\n",
      "Epoch: 611 | MAE Train Loss: 0.03773009032011032 | MAE Test Loss: 0.0886523425579071 \n",
      "Loss: 0.03769519180059433\n",
      "Epoch: 612 | MAE Train Loss: 0.03769519180059433 | MAE Test Loss: 0.08854213356971741 \n",
      "Loss: 0.03766029700636864\n",
      "Epoch: 613 | MAE Train Loss: 0.03766029700636864 | MAE Test Loss: 0.0884319469332695 \n",
      "Loss: 0.03762539103627205\n",
      "Epoch: 614 | MAE Train Loss: 0.03762539103627205 | MAE Test Loss: 0.0883217602968216 \n",
      "Loss: 0.03759048506617546\n",
      "Epoch: 615 | MAE Train Loss: 0.03759048506617546 | MAE Test Loss: 0.0882115513086319 \n",
      "Loss: 0.03755558282136917\n",
      "Epoch: 616 | MAE Train Loss: 0.03755558282136917 | MAE Test Loss: 0.0881013497710228 \n",
      "Loss: 0.03752068430185318\n",
      "Epoch: 617 | MAE Train Loss: 0.03752068430185318 | MAE Test Loss: 0.08799117058515549 \n",
      "Loss: 0.03748577460646629\n",
      "Epoch: 618 | MAE Train Loss: 0.03748577460646629 | MAE Test Loss: 0.08788096159696579 \n",
      "Loss: 0.0374508798122406\n",
      "Epoch: 619 | MAE Train Loss: 0.0374508798122406 | MAE Test Loss: 0.08777076005935669 \n",
      "Loss: 0.03741598129272461\n",
      "Epoch: 620 | MAE Train Loss: 0.03741598129272461 | MAE Test Loss: 0.08766057342290878 \n",
      "Loss: 0.03738107532262802\n",
      "Epoch: 621 | MAE Train Loss: 0.03738107532262802 | MAE Test Loss: 0.08755037188529968 \n",
      "Loss: 0.03734617680311203\n",
      "Epoch: 622 | MAE Train Loss: 0.03734617680311203 | MAE Test Loss: 0.08744016289710999 \n",
      "Loss: 0.03731127828359604\n",
      "Epoch: 623 | MAE Train Loss: 0.03731127828359604 | MAE Test Loss: 0.08732997626066208 \n",
      "Loss: 0.03727637603878975\n",
      "Epoch: 624 | MAE Train Loss: 0.03727637603878975 | MAE Test Loss: 0.08721978962421417 \n",
      "Loss: 0.03724147006869316\n",
      "Epoch: 625 | MAE Train Loss: 0.03724147006869316 | MAE Test Loss: 0.08710958063602448 \n",
      "Loss: 0.03720656782388687\n",
      "Epoch: 626 | MAE Train Loss: 0.03720656782388687 | MAE Test Loss: 0.08699937909841537 \n",
      "Loss: 0.03717166557908058\n",
      "Epoch: 627 | MAE Train Loss: 0.03717166557908058 | MAE Test Loss: 0.08688919991254807 \n",
      "Loss: 0.03713676333427429\n",
      "Epoch: 628 | MAE Train Loss: 0.03713676333427429 | MAE Test Loss: 0.08677899092435837 \n",
      "Loss: 0.0371018648147583\n",
      "Epoch: 629 | MAE Train Loss: 0.0371018648147583 | MAE Test Loss: 0.08666878938674927 \n",
      "Loss: 0.037067197263240814\n",
      "Epoch: 630 | MAE Train Loss: 0.037067197263240814 | MAE Test Loss: 0.08662726730108261 \n",
      "Loss: 0.037033338099718094\n",
      "Epoch: 631 | MAE Train Loss: 0.037033338099718094 | MAE Test Loss: 0.08651705086231232 \n",
      "Loss: 0.036998435854911804\n",
      "Epoch: 632 | MAE Train Loss: 0.036998435854911804 | MAE Test Loss: 0.08640686422586441 \n",
      "Loss: 0.036964669823646545\n",
      "Epoch: 633 | MAE Train Loss: 0.036964669823646545 | MAE Test Loss: 0.08636530488729477 \n",
      "Loss: 0.036929916590452194\n",
      "Epoch: 634 | MAE Train Loss: 0.036929916590452194 | MAE Test Loss: 0.08625511080026627 \n",
      "Loss: 0.036895766854286194\n",
      "Epoch: 635 | MAE Train Loss: 0.036895766854286194 | MAE Test Loss: 0.08621358126401901 \n",
      "Loss: 0.03686138987541199\n",
      "Epoch: 636 | MAE Train Loss: 0.03686138987541199 | MAE Test Loss: 0.08610336482524872 \n",
      "Loss: 0.036826856434345245\n",
      "Epoch: 637 | MAE Train Loss: 0.036826856434345245 | MAE Test Loss: 0.08606183528900146 \n",
      "Loss: 0.03679286316037178\n",
      "Epoch: 638 | MAE Train Loss: 0.03679286316037178 | MAE Test Loss: 0.08595161885023117 \n",
      "Loss: 0.03675796836614609\n",
      "Epoch: 639 | MAE Train Loss: 0.03675796836614609 | MAE Test Loss: 0.08584143966436386 \n",
      "Loss: 0.03672432899475098\n",
      "Epoch: 640 | MAE Train Loss: 0.03672432899475098 | MAE Test Loss: 0.08579986542463303 \n",
      "Loss: 0.03668942302465439\n",
      "Epoch: 641 | MAE Train Loss: 0.03668942302465439 | MAE Test Loss: 0.08568969368934631 \n",
      "Loss: 0.03665542230010033\n",
      "Epoch: 642 | MAE Train Loss: 0.03665542230010033 | MAE Test Loss: 0.08564814180135727 \n",
      "Loss: 0.03662090748548508\n",
      "Epoch: 643 | MAE Train Loss: 0.03662090748548508 | MAE Test Loss: 0.08553795516490936 \n",
      "Loss: 0.03658652305603027\n",
      "Epoch: 644 | MAE Train Loss: 0.03658652305603027 | MAE Test Loss: 0.08549639582633972 \n",
      "Loss: 0.03655238822102547\n",
      "Epoch: 645 | MAE Train Loss: 0.03655238822102547 | MAE Test Loss: 0.08538620173931122 \n",
      "Loss: 0.03651762008666992\n",
      "Epoch: 646 | MAE Train Loss: 0.03651762008666992 | MAE Test Loss: 0.08534465730190277 \n",
      "Loss: 0.03648385778069496\n",
      "Epoch: 647 | MAE Train Loss: 0.03648385778069496 | MAE Test Loss: 0.08523445576429367 \n",
      "Loss: 0.03644896298646927\n",
      "Epoch: 648 | MAE Train Loss: 0.03644896298646927 | MAE Test Loss: 0.08512424677610397 \n",
      "Loss: 0.03641509637236595\n",
      "Epoch: 649 | MAE Train Loss: 0.03641509637236595 | MAE Test Loss: 0.08508270978927612 \n",
      "Loss: 0.03638043254613876\n",
      "Epoch: 650 | MAE Train Loss: 0.03638043254613876 | MAE Test Loss: 0.08497253060340881 \n",
      "Loss: 0.036346182227134705\n",
      "Epoch: 651 | MAE Train Loss: 0.036346182227134705 | MAE Test Loss: 0.08493097871541977 \n",
      "Loss: 0.03631190210580826\n",
      "Epoch: 652 | MAE Train Loss: 0.03631190210580826 | MAE Test Loss: 0.08482077717781067 \n",
      "Loss: 0.03627727925777435\n",
      "Epoch: 653 | MAE Train Loss: 0.03627727925777435 | MAE Test Loss: 0.08477924019098282 \n",
      "Loss: 0.03624338284134865\n",
      "Epoch: 654 | MAE Train Loss: 0.03624338284134865 | MAE Test Loss: 0.08466903120279312 \n",
      "Loss: 0.03620847314596176\n",
      "Epoch: 655 | MAE Train Loss: 0.03620847314596176 | MAE Test Loss: 0.08455883711576462 \n",
      "Loss: 0.036174751818180084\n",
      "Epoch: 656 | MAE Train Loss: 0.036174751818180084 | MAE Test Loss: 0.08451729267835617 \n",
      "Loss: 0.03613995760679245\n",
      "Epoch: 657 | MAE Train Loss: 0.03613995760679245 | MAE Test Loss: 0.08440708369016647 \n",
      "Loss: 0.036105841398239136\n",
      "Epoch: 658 | MAE Train Loss: 0.036105841398239136 | MAE Test Loss: 0.08436556160449982 \n",
      "Loss: 0.03607143089175224\n",
      "Epoch: 659 | MAE Train Loss: 0.03607143089175224 | MAE Test Loss: 0.08425535261631012 \n",
      "Loss: 0.036036938428878784\n",
      "Epoch: 660 | MAE Train Loss: 0.036036938428878784 | MAE Test Loss: 0.08421380817890167 \n",
      "Loss: 0.036002904176712036\n",
      "Epoch: 661 | MAE Train Loss: 0.036002904176712036 | MAE Test Loss: 0.08410360664129257 \n",
      "Loss: 0.035968031734228134\n",
      "Epoch: 662 | MAE Train Loss: 0.035968031734228134 | MAE Test Loss: 0.08406206965446472 \n",
      "Loss: 0.03593437746167183\n",
      "Epoch: 663 | MAE Train Loss: 0.03593437746167183 | MAE Test Loss: 0.08395187556743622 \n",
      "Loss: 0.03589947894215584\n",
      "Epoch: 664 | MAE Train Loss: 0.03589947894215584 | MAE Test Loss: 0.08384167402982712 \n",
      "Loss: 0.035865508019924164\n",
      "Epoch: 665 | MAE Train Loss: 0.035865508019924164 | MAE Test Loss: 0.08380012214183807 \n",
      "Loss: 0.03583095222711563\n",
      "Epoch: 666 | MAE Train Loss: 0.03583095222711563 | MAE Test Loss: 0.08368994295597076 \n",
      "Loss: 0.03579659387469292\n",
      "Epoch: 667 | MAE Train Loss: 0.03579659387469292 | MAE Test Loss: 0.08364837616682053 \n",
      "Loss: 0.03576242923736572\n",
      "Epoch: 668 | MAE Train Loss: 0.03576242923736572 | MAE Test Loss: 0.08353818953037262 \n",
      "Loss: 0.03572769835591316\n",
      "Epoch: 669 | MAE Train Loss: 0.03572769835591316 | MAE Test Loss: 0.08349661529064178 \n",
      "Loss: 0.03569390997290611\n",
      "Epoch: 670 | MAE Train Loss: 0.03569390997290611 | MAE Test Loss: 0.08338643610477448 \n",
      "Loss: 0.035659000277519226\n",
      "Epoch: 671 | MAE Train Loss: 0.035659000277519226 | MAE Test Loss: 0.08327624946832657 \n",
      "Loss: 0.035625167191028595\n",
      "Epoch: 672 | MAE Train Loss: 0.035625167191028595 | MAE Test Loss: 0.08323469758033752 \n",
      "Loss: 0.03559047356247902\n",
      "Epoch: 673 | MAE Train Loss: 0.03559047356247902 | MAE Test Loss: 0.08312450349330902 \n",
      "Loss: 0.03555626422166824\n",
      "Epoch: 674 | MAE Train Loss: 0.03555626422166824 | MAE Test Loss: 0.08308294415473938 \n",
      "Loss: 0.03552195802330971\n",
      "Epoch: 675 | MAE Train Loss: 0.03552195802330971 | MAE Test Loss: 0.08297275751829147 \n",
      "Loss: 0.03548736125230789\n",
      "Epoch: 676 | MAE Train Loss: 0.03548736125230789 | MAE Test Loss: 0.08293122053146362 \n",
      "Loss: 0.0354534275829792\n",
      "Epoch: 677 | MAE Train Loss: 0.0354534275829792 | MAE Test Loss: 0.08282099664211273 \n",
      "Loss: 0.035418521612882614\n",
      "Epoch: 678 | MAE Train Loss: 0.035418521612882614 | MAE Test Loss: 0.08271081745624542 \n",
      "Loss: 0.035384830087423325\n",
      "Epoch: 679 | MAE Train Loss: 0.035384830087423325 | MAE Test Loss: 0.08266927301883698 \n",
      "Loss: 0.035350002348423004\n",
      "Epoch: 680 | MAE Train Loss: 0.035350002348423004 | MAE Test Loss: 0.08255907148122787 \n",
      "Loss: 0.035315923392772675\n",
      "Epoch: 681 | MAE Train Loss: 0.035315923392772675 | MAE Test Loss: 0.08251753449440002 \n",
      "Loss: 0.0352814719080925\n",
      "Epoch: 682 | MAE Train Loss: 0.0352814719080925 | MAE Test Loss: 0.08240732550621033 \n",
      "Loss: 0.03524700924754143\n",
      "Epoch: 683 | MAE Train Loss: 0.03524700924754143 | MAE Test Loss: 0.08236578106880188 \n",
      "Loss: 0.03521294146776199\n",
      "Epoch: 684 | MAE Train Loss: 0.03521294146776199 | MAE Test Loss: 0.08225558698177338 \n",
      "Loss: 0.03517811372876167\n",
      "Epoch: 685 | MAE Train Loss: 0.03517811372876167 | MAE Test Loss: 0.08221404999494553 \n",
      "Loss: 0.03514442220330238\n",
      "Epoch: 686 | MAE Train Loss: 0.03514442220330238 | MAE Test Loss: 0.08210383355617523 \n",
      "Loss: 0.0351095125079155\n",
      "Epoch: 687 | MAE Train Loss: 0.0351095125079155 | MAE Test Loss: 0.08199366927146912 \n",
      "Loss: 0.035075593739748\n",
      "Epoch: 688 | MAE Train Loss: 0.035075593739748 | MAE Test Loss: 0.08195210248231888 \n",
      "Loss: 0.03504099324345589\n",
      "Epoch: 689 | MAE Train Loss: 0.03504099324345589 | MAE Test Loss: 0.08184190839529037 \n",
      "Loss: 0.035006679594516754\n",
      "Epoch: 690 | MAE Train Loss: 0.035006679594516754 | MAE Test Loss: 0.08180035650730133 \n",
      "Loss: 0.03497246652841568\n",
      "Epoch: 691 | MAE Train Loss: 0.03497246652841568 | MAE Test Loss: 0.08169016242027283 \n",
      "Loss: 0.0349377766251564\n",
      "Epoch: 692 | MAE Train Loss: 0.0349377766251564 | MAE Test Loss: 0.08164862543344498 \n",
      "Loss: 0.03490393981337547\n",
      "Epoch: 693 | MAE Train Loss: 0.03490393981337547 | MAE Test Loss: 0.08153841644525528 \n",
      "Loss: 0.03486904129385948\n",
      "Epoch: 694 | MAE Train Loss: 0.03486904129385948 | MAE Test Loss: 0.08142823725938797 \n",
      "Loss: 0.03483524173498154\n",
      "Epoch: 695 | MAE Train Loss: 0.03483524173498154 | MAE Test Loss: 0.08138667792081833 \n",
      "Loss: 0.03480052202939987\n",
      "Epoch: 696 | MAE Train Loss: 0.03480052202939987 | MAE Test Loss: 0.08127648383378983 \n",
      "Loss: 0.034766342490911484\n",
      "Epoch: 697 | MAE Train Loss: 0.034766342490911484 | MAE Test Loss: 0.08123494684696198 \n",
      "Loss: 0.03473198413848877\n",
      "Epoch: 698 | MAE Train Loss: 0.03473198413848877 | MAE Test Loss: 0.08112473785877228 \n",
      "Loss: 0.034697435796260834\n",
      "Epoch: 699 | MAE Train Loss: 0.034697435796260834 | MAE Test Loss: 0.08108319342136383 \n",
      "Loss: 0.03466346859931946\n",
      "Epoch: 700 | MAE Train Loss: 0.03466346859931946 | MAE Test Loss: 0.08097299933433533 \n",
      "Loss: 0.03462856262922287\n",
      "Epoch: 701 | MAE Train Loss: 0.03462856262922287 | MAE Test Loss: 0.08086280524730682 \n",
      "Loss: 0.03459491580724716\n",
      "Epoch: 702 | MAE Train Loss: 0.03459491580724716 | MAE Test Loss: 0.08082125335931778 \n",
      "Loss: 0.03456003591418266\n",
      "Epoch: 703 | MAE Train Loss: 0.03456003591418266 | MAE Test Loss: 0.08071105927228928 \n",
      "Loss: 0.034526001662015915\n",
      "Epoch: 704 | MAE Train Loss: 0.034526001662015915 | MAE Test Loss: 0.08066950738430023 \n",
      "Loss: 0.03449151664972305\n",
      "Epoch: 705 | MAE Train Loss: 0.03449151664972305 | MAE Test Loss: 0.08055931329727173 \n",
      "Loss: 0.03445709869265556\n",
      "Epoch: 706 | MAE Train Loss: 0.03445709869265556 | MAE Test Loss: 0.08051777631044388 \n",
      "Loss: 0.03442298620939255\n",
      "Epoch: 707 | MAE Train Loss: 0.03442298620939255 | MAE Test Loss: 0.08040755987167358 \n",
      "Loss: 0.03438819572329521\n",
      "Epoch: 708 | MAE Train Loss: 0.03438819572329521 | MAE Test Loss: 0.08036604523658752 \n",
      "Loss: 0.034354470670223236\n",
      "Epoch: 709 | MAE Train Loss: 0.034354470670223236 | MAE Test Loss: 0.08025582134723663 \n",
      "Loss: 0.03431956097483635\n",
      "Epoch: 710 | MAE Train Loss: 0.03431956097483635 | MAE Test Loss: 0.08014564216136932 \n",
      "Loss: 0.03428566828370094\n",
      "Epoch: 711 | MAE Train Loss: 0.03428566828370094 | MAE Test Loss: 0.08010408282279968 \n",
      "Loss: 0.03425103425979614\n",
      "Epoch: 712 | MAE Train Loss: 0.03425103425979614 | MAE Test Loss: 0.07999388873577118 \n",
      "Loss: 0.03421676158905029\n",
      "Epoch: 713 | MAE Train Loss: 0.03421676158905029 | MAE Test Loss: 0.07995234429836273 \n",
      "Loss: 0.034182511270046234\n",
      "Epoch: 714 | MAE Train Loss: 0.034182511270046234 | MAE Test Loss: 0.07984215021133423 \n",
      "Loss: 0.034147851169109344\n",
      "Epoch: 715 | MAE Train Loss: 0.034147851169109344 | MAE Test Loss: 0.07980061322450638 \n",
      "Loss: 0.03411398082971573\n",
      "Epoch: 716 | MAE Train Loss: 0.03411398082971573 | MAE Test Loss: 0.07969042658805847 \n",
      "Loss: 0.03407908231019974\n",
      "Epoch: 717 | MAE Train Loss: 0.03407908231019974 | MAE Test Loss: 0.07958020269870758 \n",
      "Loss: 0.03404533118009567\n",
      "Epoch: 718 | MAE Train Loss: 0.03404533118009567 | MAE Test Loss: 0.07953865826129913 \n",
      "Loss: 0.03401055932044983\n",
      "Epoch: 719 | MAE Train Loss: 0.03401055932044983 | MAE Test Loss: 0.07942847162485123 \n",
      "Loss: 0.03397642448544502\n",
      "Epoch: 720 | MAE Train Loss: 0.03397642448544502 | MAE Test Loss: 0.07938691228628159 \n",
      "Loss: 0.03394203260540962\n",
      "Epoch: 721 | MAE Train Loss: 0.03394203260540962 | MAE Test Loss: 0.07927673310041428 \n",
      "Loss: 0.033907510340213776\n",
      "Epoch: 722 | MAE Train Loss: 0.033907510340213776 | MAE Test Loss: 0.07923518121242523 \n",
      "Loss: 0.03387351706624031\n",
      "Epoch: 723 | MAE Train Loss: 0.03387351706624031 | MAE Test Loss: 0.07912497967481613 \n",
      "Loss: 0.033838607370853424\n",
      "Epoch: 724 | MAE Train Loss: 0.033838607370853424 | MAE Test Loss: 0.07908343523740768 \n",
      "Loss: 0.03380498290061951\n",
      "Epoch: 725 | MAE Train Loss: 0.03380498290061951 | MAE Test Loss: 0.07897323369979858 \n",
      "Loss: 0.033770088106393814\n",
      "Epoch: 726 | MAE Train Loss: 0.033770088106393814 | MAE Test Loss: 0.07886303961277008 \n",
      "Loss: 0.033736083656549454\n",
      "Epoch: 727 | MAE Train Loss: 0.033736083656549454 | MAE Test Loss: 0.07882149517536163 \n",
      "Loss: 0.03370155766606331\n",
      "Epoch: 728 | MAE Train Loss: 0.03370155766606331 | MAE Test Loss: 0.07871129363775253 \n",
      "Loss: 0.033667173236608505\n",
      "Epoch: 729 | MAE Train Loss: 0.033667173236608505 | MAE Test Loss: 0.07866974920034409 \n",
      "Loss: 0.0336330309510231\n",
      "Epoch: 730 | MAE Train Loss: 0.0336330309510231 | MAE Test Loss: 0.07855955511331558 \n",
      "Loss: 0.033598270267248154\n",
      "Epoch: 731 | MAE Train Loss: 0.033598270267248154 | MAE Test Loss: 0.07851801812648773 \n",
      "Loss: 0.033564500510692596\n",
      "Epoch: 732 | MAE Train Loss: 0.033564500510692596 | MAE Test Loss: 0.07840781658887863 \n",
      "Loss: 0.033529605716466904\n",
      "Epoch: 733 | MAE Train Loss: 0.033529605716466904 | MAE Test Loss: 0.07829762250185013 \n",
      "Loss: 0.03349575027823448\n",
      "Epoch: 734 | MAE Train Loss: 0.03349575027823448 | MAE Test Loss: 0.07825606316328049 \n",
      "Loss: 0.0334610790014267\n",
      "Epoch: 735 | MAE Train Loss: 0.0334610790014267 | MAE Test Loss: 0.07814587652683258 \n",
      "Loss: 0.03342684358358383\n",
      "Epoch: 736 | MAE Train Loss: 0.03342684358358383 | MAE Test Loss: 0.07810431718826294 \n",
      "Loss: 0.03339255601167679\n",
      "Epoch: 737 | MAE Train Loss: 0.03339255601167679 | MAE Test Loss: 0.07799413055181503 \n",
      "Loss: 0.03335793316364288\n",
      "Epoch: 738 | MAE Train Loss: 0.03335793316364288 | MAE Test Loss: 0.07795257866382599 \n",
      "Loss: 0.03332402929663658\n",
      "Epoch: 739 | MAE Train Loss: 0.03332402929663658 | MAE Test Loss: 0.07784239202737808 \n",
      "Loss: 0.03328912705183029\n",
      "Epoch: 740 | MAE Train Loss: 0.03328912705183029 | MAE Test Loss: 0.07773219794034958 \n",
      "Loss: 0.033255405724048615\n",
      "Epoch: 741 | MAE Train Loss: 0.033255405724048615 | MAE Test Loss: 0.07769063860177994 \n",
      "Loss: 0.033220600336790085\n",
      "Epoch: 742 | MAE Train Loss: 0.033220600336790085 | MAE Test Loss: 0.07758044451475143 \n",
      "Loss: 0.03318650275468826\n",
      "Epoch: 743 | MAE Train Loss: 0.03318650275468826 | MAE Test Loss: 0.07753890007734299 \n",
      "Loss: 0.03315207362174988\n",
      "Epoch: 744 | MAE Train Loss: 0.03315207362174988 | MAE Test Loss: 0.07742870599031448 \n",
      "Loss: 0.03311759978532791\n",
      "Epoch: 745 | MAE Train Loss: 0.03311759978532791 | MAE Test Loss: 0.07738716900348663 \n",
      "Loss: 0.03308354690670967\n",
      "Epoch: 746 | MAE Train Loss: 0.03308354690670967 | MAE Test Loss: 0.07727696001529694 \n",
      "Loss: 0.033048685640096664\n",
      "Epoch: 747 | MAE Train Loss: 0.033048685640096664 | MAE Test Loss: 0.07723541557788849 \n",
      "Loss: 0.033015020191669464\n",
      "Epoch: 748 | MAE Train Loss: 0.033015020191669464 | MAE Test Loss: 0.07712522149085999 \n",
      "Loss: 0.03298012539744377\n",
      "Epoch: 749 | MAE Train Loss: 0.03298012539744377 | MAE Test Loss: 0.07701502740383148 \n",
      "Loss: 0.032946161925792694\n",
      "Epoch: 750 | MAE Train Loss: 0.032946161925792694 | MAE Test Loss: 0.07697348296642303 \n",
      "Loss: 0.032911598682403564\n",
      "Epoch: 751 | MAE Train Loss: 0.032911598682403564 | MAE Test Loss: 0.07686327397823334 \n",
      "Loss: 0.03287725895643234\n",
      "Epoch: 752 | MAE Train Loss: 0.03287725895643234 | MAE Test Loss: 0.07682173699140549 \n",
      "Loss: 0.03284307196736336\n",
      "Epoch: 753 | MAE Train Loss: 0.03284307196736336 | MAE Test Loss: 0.07671154290437698 \n",
      "Loss: 0.03280835971236229\n",
      "Epoch: 754 | MAE Train Loss: 0.03280835971236229 | MAE Test Loss: 0.07666999846696854 \n",
      "Loss: 0.03277454525232315\n",
      "Epoch: 755 | MAE Train Loss: 0.03277454525232315 | MAE Test Loss: 0.07655977457761765 \n",
      "Loss: 0.032739635556936264\n",
      "Epoch: 756 | MAE Train Loss: 0.032739635556936264 | MAE Test Loss: 0.07644960284233093 \n",
      "Loss: 0.032705824822187424\n",
      "Epoch: 757 | MAE Train Loss: 0.032705824822187424 | MAE Test Loss: 0.07640805840492249 \n",
      "Loss: 0.032671116292476654\n",
      "Epoch: 758 | MAE Train Loss: 0.032671116292476654 | MAE Test Loss: 0.07629784941673279 \n",
      "Loss: 0.03263692185282707\n",
      "Epoch: 759 | MAE Train Loss: 0.03263692185282707 | MAE Test Loss: 0.07625630497932434 \n",
      "Loss: 0.032602597028017044\n",
      "Epoch: 760 | MAE Train Loss: 0.032602597028017044 | MAE Test Loss: 0.07614611089229584 \n",
      "Loss: 0.03256801888346672\n",
      "Epoch: 761 | MAE Train Loss: 0.03256801888346672 | MAE Test Loss: 0.07610457390546799 \n",
      "Loss: 0.03253406658768654\n",
      "Epoch: 762 | MAE Train Loss: 0.03253406658768654 | MAE Test Loss: 0.07599435746669769 \n",
      "Loss: 0.032499171793460846\n",
      "Epoch: 763 | MAE Train Loss: 0.032499171793460846 | MAE Test Loss: 0.075884148478508 \n",
      "Loss: 0.03246549516916275\n",
      "Epoch: 764 | MAE Train Loss: 0.03246549516916275 | MAE Test Loss: 0.07584261894226074 \n",
      "Loss: 0.03243064135313034\n",
      "Epoch: 765 | MAE Train Loss: 0.03243064135313034 | MAE Test Loss: 0.07573243975639343 \n",
      "Loss: 0.0323965847492218\n",
      "Epoch: 766 | MAE Train Loss: 0.0323965847492218 | MAE Test Loss: 0.07569089531898499 \n",
      "Loss: 0.032362114638090134\n",
      "Epoch: 767 | MAE Train Loss: 0.032362114638090134 | MAE Test Loss: 0.07558067888021469 \n",
      "Loss: 0.03232767805457115\n",
      "Epoch: 768 | MAE Train Loss: 0.03232767805457115 | MAE Test Loss: 0.07553914934396744 \n",
      "Loss: 0.032293591648340225\n",
      "Epoch: 769 | MAE Train Loss: 0.032293591648340225 | MAE Test Loss: 0.07542894035577774 \n",
      "Loss: 0.0322587676346302\n",
      "Epoch: 770 | MAE Train Loss: 0.0322587676346302 | MAE Test Loss: 0.07538740336894989 \n",
      "Loss: 0.03222506493330002\n",
      "Epoch: 771 | MAE Train Loss: 0.03222506493330002 | MAE Test Loss: 0.07527719438076019 \n",
      "Loss: 0.03219016641378403\n",
      "Epoch: 772 | MAE Train Loss: 0.03219016641378403 | MAE Test Loss: 0.07516699284315109 \n",
      "Loss: 0.03215624764561653\n",
      "Epoch: 773 | MAE Train Loss: 0.03215624764561653 | MAE Test Loss: 0.07512547075748444 \n",
      "Loss: 0.03212163969874382\n",
      "Epoch: 774 | MAE Train Loss: 0.03212163969874382 | MAE Test Loss: 0.07501526176929474 \n",
      "Loss: 0.03208733722567558\n",
      "Epoch: 775 | MAE Train Loss: 0.03208733722567558 | MAE Test Loss: 0.07497371733188629 \n",
      "Loss: 0.03205311298370361\n",
      "Epoch: 776 | MAE Train Loss: 0.03205311298370361 | MAE Test Loss: 0.07486351579427719 \n",
      "Loss: 0.03201843053102493\n",
      "Epoch: 777 | MAE Train Loss: 0.03201843053102493 | MAE Test Loss: 0.07482197880744934 \n",
      "Loss: 0.03198458254337311\n",
      "Epoch: 778 | MAE Train Loss: 0.03198458254337311 | MAE Test Loss: 0.07471177726984024 \n",
      "Loss: 0.031949687749147415\n",
      "Epoch: 779 | MAE Train Loss: 0.031949687749147415 | MAE Test Loss: 0.07460158318281174 \n",
      "Loss: 0.031915903091430664\n",
      "Epoch: 780 | MAE Train Loss: 0.031915903091430664 | MAE Test Loss: 0.07456003129482269 \n",
      "Loss: 0.03188116103410721\n",
      "Epoch: 781 | MAE Train Loss: 0.03188116103410721 | MAE Test Loss: 0.07444985210895538 \n",
      "Loss: 0.03184700012207031\n",
      "Epoch: 782 | MAE Train Loss: 0.03184700012207031 | MAE Test Loss: 0.07440828531980515 \n",
      "Loss: 0.0318126380443573\n",
      "Epoch: 783 | MAE Train Loss: 0.0318126380443573 | MAE Test Loss: 0.07429809868335724 \n",
      "Loss: 0.03177809715270996\n",
      "Epoch: 784 | MAE Train Loss: 0.03177809715270996 | MAE Test Loss: 0.0742565244436264 \n",
      "Loss: 0.03174411877989769\n",
      "Epoch: 785 | MAE Train Loss: 0.03174411877989769 | MAE Test Loss: 0.0741463452577591 \n",
      "Loss: 0.0317092090845108\n",
      "Epoch: 786 | MAE Train Loss: 0.0317092090845108 | MAE Test Loss: 0.07403616607189178 \n",
      "Loss: 0.03167556971311569\n",
      "Epoch: 787 | MAE Train Loss: 0.03167556971311569 | MAE Test Loss: 0.07399461418390274 \n",
      "Loss: 0.031640682369470596\n",
      "Epoch: 788 | MAE Train Loss: 0.031640682369470596 | MAE Test Loss: 0.07388440519571304 \n",
      "Loss: 0.03160666301846504\n",
      "Epoch: 789 | MAE Train Loss: 0.03160666301846504 | MAE Test Loss: 0.073842853307724 \n",
      "Loss: 0.031572163105010986\n",
      "Epoch: 790 | MAE Train Loss: 0.031572163105010986 | MAE Test Loss: 0.07373266667127609 \n",
      "Loss: 0.03153776004910469\n",
      "Epoch: 791 | MAE Train Loss: 0.03153776004910469 | MAE Test Loss: 0.07369112968444824 \n",
      "Loss: 0.03150363638997078\n",
      "Epoch: 792 | MAE Train Loss: 0.03150363638997078 | MAE Test Loss: 0.07358090579509735 \n",
      "Loss: 0.03146884962916374\n",
      "Epoch: 793 | MAE Train Loss: 0.03146884962916374 | MAE Test Loss: 0.0735393688082695 \n",
      "Loss: 0.03143510967493057\n",
      "Epoch: 794 | MAE Train Loss: 0.03143510967493057 | MAE Test Loss: 0.0734291821718216 \n",
      "Loss: 0.03140021115541458\n",
      "Epoch: 795 | MAE Train Loss: 0.03140021115541458 | MAE Test Loss: 0.0733189806342125 \n",
      "Loss: 0.03136632591485977\n",
      "Epoch: 796 | MAE Train Loss: 0.03136632591485977 | MAE Test Loss: 0.07327743619680405 \n",
      "Loss: 0.031331680715084076\n",
      "Epoch: 797 | MAE Train Loss: 0.031331680715084076 | MAE Test Loss: 0.07316723465919495 \n",
      "Loss: 0.031297408044338226\n",
      "Epoch: 798 | MAE Train Loss: 0.031297408044338226 | MAE Test Loss: 0.0731256827712059 \n",
      "Loss: 0.03126315027475357\n",
      "Epoch: 799 | MAE Train Loss: 0.03126315027475357 | MAE Test Loss: 0.073015496134758 \n",
      "Loss: 0.03122851625084877\n",
      "Epoch: 800 | MAE Train Loss: 0.03122851625084877 | MAE Test Loss: 0.07297395914793015 \n",
      "Loss: 0.03119463101029396\n",
      "Epoch: 801 | MAE Train Loss: 0.03119463101029396 | MAE Test Loss: 0.07286374270915985 \n",
      "Loss: 0.031159725040197372\n",
      "Epoch: 802 | MAE Train Loss: 0.031159725040197372 | MAE Test Loss: 0.07275357842445374 \n",
      "Loss: 0.0311259925365448\n",
      "Epoch: 803 | MAE Train Loss: 0.0311259925365448 | MAE Test Loss: 0.0727120116353035 \n",
      "Loss: 0.031091203913092613\n",
      "Epoch: 804 | MAE Train Loss: 0.031091203913092613 | MAE Test Loss: 0.07260182499885559 \n",
      "Loss: 0.031057080253958702\n",
      "Epoch: 805 | MAE Train Loss: 0.031057080253958702 | MAE Test Loss: 0.07256026566028595 \n",
      "Loss: 0.031022679060697556\n",
      "Epoch: 806 | MAE Train Loss: 0.031022679060697556 | MAE Test Loss: 0.07245006412267685 \n",
      "Loss: 0.03098817728459835\n",
      "Epoch: 807 | MAE Train Loss: 0.03098817728459835 | MAE Test Loss: 0.07240854203701019 \n",
      "Loss: 0.03095415234565735\n",
      "Epoch: 808 | MAE Train Loss: 0.03095415234565735 | MAE Test Loss: 0.0722983330488205 \n",
      "Loss: 0.03091926872730255\n",
      "Epoch: 809 | MAE Train Loss: 0.03091926872730255 | MAE Test Loss: 0.07225678116083145 \n",
      "Loss: 0.03088562563061714\n",
      "Epoch: 810 | MAE Train Loss: 0.03088562563061714 | MAE Test Loss: 0.07214658707380295 \n",
      "Loss: 0.0308507289737463\n",
      "Epoch: 811 | MAE Train Loss: 0.0308507289737463 | MAE Test Loss: 0.07203639298677444 \n",
      "Loss: 0.03081674501299858\n",
      "Epoch: 812 | MAE Train Loss: 0.03081674501299858 | MAE Test Loss: 0.0719948559999466 \n",
      "Loss: 0.030782198533415794\n",
      "Epoch: 813 | MAE Train Loss: 0.030782198533415794 | MAE Test Loss: 0.0718846470117569 \n",
      "Loss: 0.03074783645570278\n",
      "Epoch: 814 | MAE Train Loss: 0.03074783645570278 | MAE Test Loss: 0.07184310257434845 \n",
      "Loss: 0.030713677406311035\n",
      "Epoch: 815 | MAE Train Loss: 0.030713677406311035 | MAE Test Loss: 0.07173290848731995 \n",
      "Loss: 0.03067893162369728\n",
      "Epoch: 816 | MAE Train Loss: 0.03067893162369728 | MAE Test Loss: 0.0716913565993309 \n",
      "Loss: 0.030645150691270828\n",
      "Epoch: 817 | MAE Train Loss: 0.030645150691270828 | MAE Test Loss: 0.071581169962883 \n",
      "Loss: 0.03061024472117424\n",
      "Epoch: 818 | MAE Train Loss: 0.03061024472117424 | MAE Test Loss: 0.0714709609746933 \n",
      "Loss: 0.030576402321457863\n",
      "Epoch: 819 | MAE Train Loss: 0.030576402321457863 | MAE Test Loss: 0.07142942398786545 \n",
      "Loss: 0.03054172359406948\n",
      "Epoch: 820 | MAE Train Loss: 0.03054172359406948 | MAE Test Loss: 0.07131922990083694 \n",
      "Loss: 0.03050749935209751\n",
      "Epoch: 821 | MAE Train Loss: 0.03050749935209751 | MAE Test Loss: 0.0712776854634285 \n",
      "Loss: 0.030473193153738976\n",
      "Epoch: 822 | MAE Train Loss: 0.030473193153738976 | MAE Test Loss: 0.0711674615740776 \n",
      "Loss: 0.03043859638273716\n",
      "Epoch: 823 | MAE Train Loss: 0.03043859638273716 | MAE Test Loss: 0.07112595438957214 \n",
      "Loss: 0.030404681339859962\n",
      "Epoch: 824 | MAE Train Loss: 0.030404681339859962 | MAE Test Loss: 0.07101573050022125 \n",
      "Loss: 0.030369769781827927\n",
      "Epoch: 825 | MAE Train Loss: 0.030369769781827927 | MAE Test Loss: 0.07090555131435394 \n",
      "Loss: 0.030336063355207443\n",
      "Epoch: 826 | MAE Train Loss: 0.030336063355207443 | MAE Test Loss: 0.0708639919757843 \n",
      "Loss: 0.03030124306678772\n",
      "Epoch: 827 | MAE Train Loss: 0.03030124306678772 | MAE Test Loss: 0.0707537978887558 \n",
      "Loss: 0.030267158523201942\n",
      "Epoch: 828 | MAE Train Loss: 0.030267158523201942 | MAE Test Loss: 0.07071225345134735 \n",
      "Loss: 0.03023272193968296\n",
      "Epoch: 829 | MAE Train Loss: 0.03023272193968296 | MAE Test Loss: 0.07060205936431885 \n",
      "Loss: 0.030198251828551292\n",
      "Epoch: 830 | MAE Train Loss: 0.030198251828551292 | MAE Test Loss: 0.0705605149269104 \n",
      "Loss: 0.030164187774062157\n",
      "Epoch: 831 | MAE Train Loss: 0.030164187774062157 | MAE Test Loss: 0.07045033574104309 \n",
      "Loss: 0.030129343271255493\n",
      "Epoch: 832 | MAE Train Loss: 0.030129343271255493 | MAE Test Loss: 0.07040876895189285 \n",
      "Loss: 0.030095672234892845\n",
      "Epoch: 833 | MAE Train Loss: 0.030095672234892845 | MAE Test Loss: 0.07029856741428375 \n",
      "Loss: 0.030060768127441406\n",
      "Epoch: 834 | MAE Train Loss: 0.030060768127441406 | MAE Test Loss: 0.07018838077783585 \n",
      "Loss: 0.03002682328224182\n",
      "Epoch: 835 | MAE Train Loss: 0.03002682328224182 | MAE Test Loss: 0.0701468288898468 \n",
      "Loss: 0.0299922414124012\n",
      "Epoch: 836 | MAE Train Loss: 0.0299922414124012 | MAE Test Loss: 0.07003664970397949 \n",
      "Loss: 0.029957914724946022\n",
      "Epoch: 837 | MAE Train Loss: 0.029957914724946022 | MAE Test Loss: 0.06999509036540985 \n",
      "Loss: 0.02992371842265129\n",
      "Epoch: 838 | MAE Train Loss: 0.02992371842265129 | MAE Test Loss: 0.06988489627838135 \n",
      "Loss: 0.02988901175558567\n",
      "Epoch: 839 | MAE Train Loss: 0.02988901175558567 | MAE Test Loss: 0.0698433369398117 \n",
      "Loss: 0.029855191707611084\n",
      "Epoch: 840 | MAE Train Loss: 0.029855191707611084 | MAE Test Loss: 0.0697331428527832 \n",
      "Loss: 0.029820293188095093\n",
      "Epoch: 841 | MAE Train Loss: 0.029820293188095093 | MAE Test Loss: 0.0696229487657547 \n",
      "Loss: 0.029786482453346252\n",
      "Epoch: 842 | MAE Train Loss: 0.029786482453346252 | MAE Test Loss: 0.06958140432834625 \n",
      "Loss: 0.029751762747764587\n",
      "Epoch: 843 | MAE Train Loss: 0.029751762747764587 | MAE Test Loss: 0.06947120279073715 \n",
      "Loss: 0.029717573896050453\n",
      "Epoch: 844 | MAE Train Loss: 0.029717573896050453 | MAE Test Loss: 0.0694296583533287 \n",
      "Loss: 0.029683241620659828\n",
      "Epoch: 845 | MAE Train Loss: 0.029683241620659828 | MAE Test Loss: 0.0693194642663002 \n",
      "Loss: 0.0296486709266901\n",
      "Epoch: 846 | MAE Train Loss: 0.0296486709266901 | MAE Test Loss: 0.06927792727947235 \n",
      "Loss: 0.029614711180329323\n",
      "Epoch: 847 | MAE Train Loss: 0.029614711180329323 | MAE Test Loss: 0.06916771829128265 \n",
      "Loss: 0.02957981452345848\n",
      "Epoch: 848 | MAE Train Loss: 0.02957981452345848 | MAE Test Loss: 0.06905753165483475 \n",
      "Loss: 0.02954614721238613\n",
      "Epoch: 849 | MAE Train Loss: 0.02954614721238613 | MAE Test Loss: 0.0690159797668457 \n",
      "Loss: 0.029511287808418274\n",
      "Epoch: 850 | MAE Train Loss: 0.029511287808418274 | MAE Test Loss: 0.0689057856798172 \n",
      "Loss: 0.02947724238038063\n",
      "Epoch: 851 | MAE Train Loss: 0.02947724238038063 | MAE Test Loss: 0.06886423379182816 \n",
      "Loss: 0.029442762956023216\n",
      "Epoch: 852 | MAE Train Loss: 0.029442762956023216 | MAE Test Loss: 0.06875403970479965 \n",
      "Loss: 0.029408331960439682\n",
      "Epoch: 853 | MAE Train Loss: 0.029408331960439682 | MAE Test Loss: 0.06871248781681061 \n",
      "Loss: 0.02937423810362816\n",
      "Epoch: 854 | MAE Train Loss: 0.02937423810362816 | MAE Test Loss: 0.0686022937297821 \n",
      "Loss: 0.02933942899107933\n",
      "Epoch: 855 | MAE Train Loss: 0.02933942899107933 | MAE Test Loss: 0.06856075674295425 \n",
      "Loss: 0.02930571138858795\n",
      "Epoch: 856 | MAE Train Loss: 0.02930571138858795 | MAE Test Loss: 0.06845054775476456 \n",
      "Loss: 0.029270809143781662\n",
      "Epoch: 857 | MAE Train Loss: 0.029270809143781662 | MAE Test Loss: 0.06834035366773605 \n",
      "Loss: 0.02923690341413021\n",
      "Epoch: 858 | MAE Train Loss: 0.02923690341413021 | MAE Test Loss: 0.0682988092303276 \n",
      "Loss: 0.029202282428741455\n",
      "Epoch: 859 | MAE Train Loss: 0.029202282428741455 | MAE Test Loss: 0.0681886151432991 \n",
      "Loss: 0.02916799858212471\n",
      "Epoch: 860 | MAE Train Loss: 0.02916799858212471 | MAE Test Loss: 0.06814707070589066 \n",
      "Loss: 0.029133755713701248\n",
      "Epoch: 861 | MAE Train Loss: 0.029133755713701248 | MAE Test Loss: 0.06803686916828156 \n",
      "Loss: 0.029099086299538612\n",
      "Epoch: 862 | MAE Train Loss: 0.029099086299538612 | MAE Test Loss: 0.06799532473087311 \n",
      "Loss: 0.029065227136015892\n",
      "Epoch: 863 | MAE Train Loss: 0.029065227136015892 | MAE Test Loss: 0.0678851306438446 \n",
      "Loss: 0.02903033420443535\n",
      "Epoch: 864 | MAE Train Loss: 0.02903033420443535 | MAE Test Loss: 0.0677749365568161 \n",
      "Loss: 0.028996562585234642\n",
      "Epoch: 865 | MAE Train Loss: 0.028996562585234642 | MAE Test Loss: 0.06773339211940765 \n",
      "Loss: 0.02896180748939514\n",
      "Epoch: 866 | MAE Train Loss: 0.02896180748939514 | MAE Test Loss: 0.06762318313121796 \n",
      "Loss: 0.02892765775322914\n",
      "Epoch: 867 | MAE Train Loss: 0.02892765775322914 | MAE Test Loss: 0.0675816461443901 \n",
      "Loss: 0.028893280774354935\n",
      "Epoch: 868 | MAE Train Loss: 0.028893280774354935 | MAE Test Loss: 0.0674714595079422 \n",
      "Loss: 0.02885875664651394\n",
      "Epoch: 869 | MAE Train Loss: 0.02885875664651394 | MAE Test Loss: 0.06742990016937256 \n",
      "Loss: 0.028824755921959877\n",
      "Epoch: 870 | MAE Train Loss: 0.028824755921959877 | MAE Test Loss: 0.06731969118118286 \n",
      "Loss: 0.02878984436392784\n",
      "Epoch: 871 | MAE Train Loss: 0.02878984436392784 | MAE Test Loss: 0.06720949709415436 \n",
      "Loss: 0.02875622548162937\n",
      "Epoch: 872 | MAE Train Loss: 0.02875622548162937 | MAE Test Loss: 0.0671679675579071 \n",
      "Loss: 0.02872132696211338\n",
      "Epoch: 873 | MAE Train Loss: 0.02872132696211338 | MAE Test Loss: 0.06705774366855621 \n",
      "Loss: 0.02868732251226902\n",
      "Epoch: 874 | MAE Train Loss: 0.02868732251226902 | MAE Test Loss: 0.06701621413230896 \n",
      "Loss: 0.02865280583500862\n",
      "Epoch: 875 | MAE Train Loss: 0.02865280583500862 | MAE Test Loss: 0.06690602004528046 \n",
      "Loss: 0.02861841954290867\n",
      "Epoch: 876 | MAE Train Loss: 0.02861841954290867 | MAE Test Loss: 0.0668644830584526 \n",
      "Loss: 0.028584275394678116\n",
      "Epoch: 877 | MAE Train Loss: 0.028584275394678116 | MAE Test Loss: 0.06675426661968231 \n",
      "Loss: 0.02854951098561287\n",
      "Epoch: 878 | MAE Train Loss: 0.02854951098561287 | MAE Test Loss: 0.06671272963285446 \n",
      "Loss: 0.028515752404928207\n",
      "Epoch: 879 | MAE Train Loss: 0.028515752404928207 | MAE Test Loss: 0.06660252809524536 \n",
      "Loss: 0.028480852022767067\n",
      "Epoch: 880 | MAE Train Loss: 0.028480852022767067 | MAE Test Loss: 0.06649234890937805 \n",
      "Loss: 0.0284469835460186\n",
      "Epoch: 881 | MAE Train Loss: 0.0284469835460186 | MAE Test Loss: 0.06645079702138901 \n",
      "Loss: 0.02841232344508171\n",
      "Epoch: 882 | MAE Train Loss: 0.02841232344508171 | MAE Test Loss: 0.06634058058261871 \n",
      "Loss: 0.0283780749887228\n",
      "Epoch: 883 | MAE Train Loss: 0.0283780749887228 | MAE Test Loss: 0.06629905849695206 \n",
      "Loss: 0.028343800455331802\n",
      "Epoch: 884 | MAE Train Loss: 0.028343800455331802 | MAE Test Loss: 0.06618884950876236 \n",
      "Loss: 0.02830917201936245\n",
      "Epoch: 885 | MAE Train Loss: 0.02830917201936245 | MAE Test Loss: 0.06614731252193451 \n",
      "Loss: 0.028275271877646446\n",
      "Epoch: 886 | MAE Train Loss: 0.028275271877646446 | MAE Test Loss: 0.06603710353374481 \n",
      "Loss: 0.028240377083420753\n",
      "Epoch: 887 | MAE Train Loss: 0.028240377083420753 | MAE Test Loss: 0.06592690199613571 \n",
      "Loss: 0.02820664644241333\n",
      "Epoch: 888 | MAE Train Loss: 0.02820664644241333 | MAE Test Loss: 0.06588537245988846 \n",
      "Loss: 0.028171848505735397\n",
      "Epoch: 889 | MAE Train Loss: 0.028171848505735397 | MAE Test Loss: 0.06577517092227936 \n",
      "Loss: 0.02813773788511753\n",
      "Epoch: 890 | MAE Train Loss: 0.02813773788511753 | MAE Test Loss: 0.06573362648487091 \n",
      "Loss: 0.028103318065404892\n",
      "Epoch: 891 | MAE Train Loss: 0.028103318065404892 | MAE Test Loss: 0.06562343239784241 \n",
      "Loss: 0.02806883119046688\n",
      "Epoch: 892 | MAE Train Loss: 0.02806883119046688 | MAE Test Loss: 0.06558187305927277 \n",
      "Loss: 0.028034795075654984\n",
      "Epoch: 893 | MAE Train Loss: 0.028034795075654984 | MAE Test Loss: 0.06547168642282486 \n",
      "Loss: 0.02799992822110653\n",
      "Epoch: 894 | MAE Train Loss: 0.02799992822110653 | MAE Test Loss: 0.06543011963367462 \n",
      "Loss: 0.027966272085905075\n",
      "Epoch: 895 | MAE Train Loss: 0.027966272085905075 | MAE Test Loss: 0.06531994044780731 \n",
      "Loss: 0.027931367978453636\n",
      "Epoch: 896 | MAE Train Loss: 0.027931367978453636 | MAE Test Loss: 0.0652097538113594 \n",
      "Loss: 0.02789739891886711\n",
      "Epoch: 897 | MAE Train Loss: 0.02789739891886711 | MAE Test Loss: 0.06516819447278976 \n",
      "Loss: 0.027862846851348877\n",
      "Epoch: 898 | MAE Train Loss: 0.027862846851348877 | MAE Test Loss: 0.06505800783634186 \n",
      "Loss: 0.02782849594950676\n",
      "Epoch: 899 | MAE Train Loss: 0.02782849594950676 | MAE Test Loss: 0.06501643359661102 \n",
      "Loss: 0.027794327586889267\n",
      "Epoch: 900 | MAE Train Loss: 0.027794327586889267 | MAE Test Loss: 0.06490625441074371 \n",
      "Loss: 0.02775959111750126\n",
      "Epoch: 901 | MAE Train Loss: 0.02775959111750126 | MAE Test Loss: 0.06486472487449646 \n",
      "Loss: 0.027725795283913612\n",
      "Epoch: 902 | MAE Train Loss: 0.027725795283913612 | MAE Test Loss: 0.06475453078746796 \n",
      "Loss: 0.027690893039107323\n",
      "Epoch: 903 | MAE Train Loss: 0.027690893039107323 | MAE Test Loss: 0.06464431434869766 \n",
      "Loss: 0.02765706181526184\n",
      "Epoch: 904 | MAE Train Loss: 0.02765706181526184 | MAE Test Loss: 0.06460276246070862 \n",
      "Loss: 0.027622371912002563\n",
      "Epoch: 905 | MAE Train Loss: 0.027622371912002563 | MAE Test Loss: 0.06449256837368011 \n",
      "Loss: 0.02758815884590149\n",
      "Epoch: 906 | MAE Train Loss: 0.02758815884590149 | MAE Test Loss: 0.06445103138685226 \n",
      "Loss: 0.027553845196962357\n",
      "Epoch: 907 | MAE Train Loss: 0.027553845196962357 | MAE Test Loss: 0.06434081494808197 \n",
      "Loss: 0.02751925028860569\n",
      "Epoch: 908 | MAE Train Loss: 0.02751925028860569 | MAE Test Loss: 0.06429927796125412 \n",
      "Loss: 0.027485316619277\n",
      "Epoch: 909 | MAE Train Loss: 0.027485316619277 | MAE Test Loss: 0.06418909132480621 \n",
      "Loss: 0.02745041809976101\n",
      "Epoch: 910 | MAE Train Loss: 0.02745041809976101 | MAE Test Loss: 0.06407888978719711 \n",
      "Loss: 0.02741672657430172\n",
      "Epoch: 911 | MAE Train Loss: 0.02741672657430172 | MAE Test Loss: 0.06403734534978867 \n",
      "Loss: 0.027381891384720802\n",
      "Epoch: 912 | MAE Train Loss: 0.027381891384720802 | MAE Test Loss: 0.06392714381217957 \n",
      "Loss: 0.027347808703780174\n",
      "Epoch: 913 | MAE Train Loss: 0.027347808703780174 | MAE Test Loss: 0.06388559192419052 \n",
      "Loss: 0.027313362807035446\n",
      "Epoch: 914 | MAE Train Loss: 0.027313362807035446 | MAE Test Loss: 0.06377541273832321 \n",
      "Loss: 0.02727891504764557\n",
      "Epoch: 915 | MAE Train Loss: 0.02727891504764557 | MAE Test Loss: 0.06373386085033417 \n",
      "Loss: 0.02724483609199524\n",
      "Epoch: 916 | MAE Train Loss: 0.02724483609199524 | MAE Test Loss: 0.06362365186214447 \n",
      "Loss: 0.02721000649034977\n",
      "Epoch: 917 | MAE Train Loss: 0.02721000649034977 | MAE Test Loss: 0.06358212977647781 \n",
      "Loss: 0.02717631123960018\n",
      "Epoch: 918 | MAE Train Loss: 0.02717631123960018 | MAE Test Loss: 0.06347192078828812 \n",
      "Loss: 0.02714141272008419\n",
      "Epoch: 919 | MAE Train Loss: 0.02714141272008419 | MAE Test Loss: 0.06336172670125961 \n",
      "Loss: 0.0271074827760458\n",
      "Epoch: 920 | MAE Train Loss: 0.0271074827760458 | MAE Test Loss: 0.06332017481327057 \n",
      "Loss: 0.027072886005043983\n",
      "Epoch: 921 | MAE Train Loss: 0.027072886005043983 | MAE Test Loss: 0.06320997327566147 \n",
      "Loss: 0.0270385779440403\n",
      "Epoch: 922 | MAE Train Loss: 0.0270385779440403 | MAE Test Loss: 0.06316845118999481 \n",
      "Loss: 0.027004361152648926\n",
      "Epoch: 923 | MAE Train Loss: 0.027004361152648926 | MAE Test Loss: 0.06305824220180511 \n",
      "Loss: 0.02696967124938965\n",
      "Epoch: 924 | MAE Train Loss: 0.02696967124938965 | MAE Test Loss: 0.06301669031381607 \n",
      "Loss: 0.02693583443760872\n",
      "Epoch: 925 | MAE Train Loss: 0.02693583443760872 | MAE Test Loss: 0.06290650367736816 \n",
      "Loss: 0.026900935918092728\n",
      "Epoch: 926 | MAE Train Loss: 0.026900935918092728 | MAE Test Loss: 0.06279630213975906 \n",
      "Loss: 0.02686714567244053\n",
      "Epoch: 927 | MAE Train Loss: 0.02686714567244053 | MAE Test Loss: 0.06275476515293121 \n",
      "Loss: 0.02683240734040737\n",
      "Epoch: 928 | MAE Train Loss: 0.02683240734040737 | MAE Test Loss: 0.06264455616474152 \n",
      "Loss: 0.02679823711514473\n",
      "Epoch: 929 | MAE Train Loss: 0.02679823711514473 | MAE Test Loss: 0.06260301172733307 \n",
      "Loss: 0.026763886213302612\n",
      "Epoch: 930 | MAE Train Loss: 0.026763886213302612 | MAE Test Loss: 0.062492817640304565 \n",
      "Loss: 0.02672933042049408\n",
      "Epoch: 931 | MAE Train Loss: 0.02672933042049408 | MAE Test Loss: 0.06245126575231552 \n",
      "Loss: 0.026695361360907555\n",
      "Epoch: 932 | MAE Train Loss: 0.026695361360907555 | MAE Test Loss: 0.06234106421470642 \n",
      "Loss: 0.026660453528165817\n",
      "Epoch: 933 | MAE Train Loss: 0.026660453528165817 | MAE Test Loss: 0.062230873852968216 \n",
      "Loss: 0.02662680111825466\n",
      "Epoch: 934 | MAE Train Loss: 0.02662680111825466 | MAE Test Loss: 0.06218933314085007 \n",
      "Loss: 0.026591932401061058\n",
      "Epoch: 935 | MAE Train Loss: 0.026591932401061058 | MAE Test Loss: 0.062079139053821564 \n",
      "Loss: 0.02655789814889431\n",
      "Epoch: 936 | MAE Train Loss: 0.02655789814889431 | MAE Test Loss: 0.062037594616413116 \n",
      "Loss: 0.026523401960730553\n",
      "Epoch: 937 | MAE Train Loss: 0.026523401960730553 | MAE Test Loss: 0.06192737817764282 \n",
      "Loss: 0.026488998904824257\n",
      "Epoch: 938 | MAE Train Loss: 0.026488998904824257 | MAE Test Loss: 0.061885856091976166 \n",
      "Loss: 0.02645489014685154\n",
      "Epoch: 939 | MAE Train Loss: 0.02645489014685154 | MAE Test Loss: 0.06177564337849617 \n",
      "Loss: 0.02642008289694786\n",
      "Epoch: 940 | MAE Train Loss: 0.02642008289694786 | MAE Test Loss: 0.06173409894108772 \n",
      "Loss: 0.026386354118585587\n",
      "Epoch: 941 | MAE Train Loss: 0.026386354118585587 | MAE Test Loss: 0.06162390112876892 \n",
      "Loss: 0.026351451873779297\n",
      "Epoch: 942 | MAE Train Loss: 0.026351451873779297 | MAE Test Loss: 0.06151369959115982 \n",
      "Loss: 0.02631755731999874\n",
      "Epoch: 943 | MAE Train Loss: 0.02631755731999874 | MAE Test Loss: 0.06147215515375137 \n",
      "Loss: 0.026282930746674538\n",
      "Epoch: 944 | MAE Train Loss: 0.026282930746674538 | MAE Test Loss: 0.061361975967884064 \n",
      "Loss: 0.02624865248799324\n",
      "Epoch: 945 | MAE Train Loss: 0.02624865248799324 | MAE Test Loss: 0.06132042407989502 \n",
      "Loss: 0.026214396581053734\n",
      "Epoch: 946 | MAE Train Loss: 0.026214396581053734 | MAE Test Loss: 0.06121024489402771 \n",
      "Loss: 0.026179742068052292\n",
      "Epoch: 947 | MAE Train Loss: 0.026179742068052292 | MAE Test Loss: 0.061168670654296875 \n",
      "Loss: 0.026145881041884422\n",
      "Epoch: 948 | MAE Train Loss: 0.026145881041884422 | MAE Test Loss: 0.06105848029255867 \n",
      "Loss: 0.026110976934432983\n",
      "Epoch: 949 | MAE Train Loss: 0.026110976934432983 | MAE Test Loss: 0.060948289930820465 \n",
      "Loss: 0.02607722207903862\n",
      "Epoch: 950 | MAE Train Loss: 0.02607722207903862 | MAE Test Loss: 0.06090673804283142 \n",
      "Loss: 0.026042450219392776\n",
      "Epoch: 951 | MAE Train Loss: 0.026042450219392776 | MAE Test Loss: 0.060796551406383514 \n",
      "Loss: 0.02600831352174282\n",
      "Epoch: 952 | MAE Train Loss: 0.02600831352174282 | MAE Test Loss: 0.06075499206781387 \n",
      "Loss: 0.025973927229642868\n",
      "Epoch: 953 | MAE Train Loss: 0.025973927229642868 | MAE Test Loss: 0.06064480543136597 \n",
      "Loss: 0.02593941055238247\n",
      "Epoch: 954 | MAE Train Loss: 0.02593941055238247 | MAE Test Loss: 0.060603249818086624 \n",
      "Loss: 0.02590540051460266\n",
      "Epoch: 955 | MAE Train Loss: 0.02590540051460266 | MAE Test Loss: 0.06049305200576782 \n",
      "Loss: 0.025870507583022118\n",
      "Epoch: 956 | MAE Train Loss: 0.025870507583022118 | MAE Test Loss: 0.06045151501893997 \n",
      "Loss: 0.025836873799562454\n",
      "Epoch: 957 | MAE Train Loss: 0.025836873799562454 | MAE Test Loss: 0.06034131720662117 \n",
      "Loss: 0.025801971554756165\n",
      "Epoch: 958 | MAE Train Loss: 0.025801971554756165 | MAE Test Loss: 0.06023111194372177 \n",
      "Loss: 0.025767970830202103\n",
      "Epoch: 959 | MAE Train Loss: 0.025767970830202103 | MAE Test Loss: 0.060189567506313324 \n",
      "Loss: 0.025733450427651405\n",
      "Epoch: 960 | MAE Train Loss: 0.025733450427651405 | MAE Test Loss: 0.06007937341928482 \n",
      "Loss: 0.0256990734487772\n",
      "Epoch: 961 | MAE Train Loss: 0.0256990734487772 | MAE Test Loss: 0.06003783270716667 \n",
      "Loss: 0.0256649199873209\n",
      "Epoch: 962 | MAE Train Loss: 0.0256649199873209 | MAE Test Loss: 0.05992763116955757 \n",
      "Loss: 0.025630170479416847\n",
      "Epoch: 963 | MAE Train Loss: 0.025630170479416847 | MAE Test Loss: 0.05988607555627823 \n",
      "Loss: 0.025596395134925842\n",
      "Epoch: 964 | MAE Train Loss: 0.025596395134925842 | MAE Test Loss: 0.05977589637041092 \n",
      "Loss: 0.02556149661540985\n",
      "Epoch: 965 | MAE Train Loss: 0.02556149661540985 | MAE Test Loss: 0.05966568738222122 \n",
      "Loss: 0.02552764117717743\n",
      "Epoch: 966 | MAE Train Loss: 0.02552764117717743 | MAE Test Loss: 0.05962413549423218 \n",
      "Loss: 0.025492969900369644\n",
      "Epoch: 967 | MAE Train Loss: 0.025492969900369644 | MAE Test Loss: 0.05951394885778427 \n",
      "Loss: 0.02545873448252678\n",
      "Epoch: 968 | MAE Train Loss: 0.02545873448252678 | MAE Test Loss: 0.059472404420375824 \n",
      "Loss: 0.025424445047974586\n",
      "Epoch: 969 | MAE Train Loss: 0.025424445047974586 | MAE Test Loss: 0.059362202882766724 \n",
      "Loss: 0.02538982965052128\n",
      "Epoch: 970 | MAE Train Loss: 0.02538982965052128 | MAE Test Loss: 0.05932066589593887 \n",
      "Loss: 0.02535592019557953\n",
      "Epoch: 971 | MAE Train Loss: 0.02535592019557953 | MAE Test Loss: 0.059210460633039474 \n",
      "Loss: 0.02532101795077324\n",
      "Epoch: 972 | MAE Train Loss: 0.02532101795077324 | MAE Test Loss: 0.05910026282072067 \n",
      "Loss: 0.02528730407357216\n",
      "Epoch: 973 | MAE Train Loss: 0.02528730407357216 | MAE Test Loss: 0.059058718383312225 \n",
      "Loss: 0.025252491235733032\n",
      "Epoch: 974 | MAE Train Loss: 0.025252491235733032 | MAE Test Loss: 0.058948516845703125 \n",
      "Loss: 0.02521839737892151\n",
      "Epoch: 975 | MAE Train Loss: 0.02521839737892151 | MAE Test Loss: 0.05890697240829468 \n",
      "Loss: 0.025183964520692825\n",
      "Epoch: 976 | MAE Train Loss: 0.025183964520692825 | MAE Test Loss: 0.058796774595975876 \n",
      "Loss: 0.02514948509633541\n",
      "Epoch: 977 | MAE Train Loss: 0.02514948509633541 | MAE Test Loss: 0.05875522643327713 \n",
      "Loss: 0.025115439668297768\n",
      "Epoch: 978 | MAE Train Loss: 0.025115439668297768 | MAE Test Loss: 0.058645039796829224 \n",
      "Loss: 0.025080587714910507\n",
      "Epoch: 979 | MAE Train Loss: 0.025080587714910507 | MAE Test Loss: 0.05860348790884018 \n",
      "Loss: 0.025046909227967262\n",
      "Epoch: 980 | MAE Train Loss: 0.025046909227967262 | MAE Test Loss: 0.058493297547101974 \n",
      "Loss: 0.02501201629638672\n",
      "Epoch: 981 | MAE Train Loss: 0.02501201629638672 | MAE Test Loss: 0.058383096009492874 \n",
      "Loss: 0.02497805655002594\n",
      "Epoch: 982 | MAE Train Loss: 0.02497805655002594 | MAE Test Loss: 0.058341555297374725 \n",
      "Loss: 0.024943489581346512\n",
      "Epoch: 983 | MAE Train Loss: 0.024943489581346512 | MAE Test Loss: 0.05823137238621712 \n",
      "Loss: 0.024909157305955887\n",
      "Epoch: 984 | MAE Train Loss: 0.024909157305955887 | MAE Test Loss: 0.05818980187177658 \n",
      "Loss: 0.024874964728951454\n",
      "Epoch: 985 | MAE Train Loss: 0.024874964728951454 | MAE Test Loss: 0.05807960033416748 \n",
      "Loss: 0.02484023943543434\n",
      "Epoch: 986 | MAE Train Loss: 0.02484023943543434 | MAE Test Loss: 0.05803806334733963 \n",
      "Loss: 0.024806439876556396\n",
      "Epoch: 987 | MAE Train Loss: 0.024806439876556396 | MAE Test Loss: 0.057927876710891724 \n",
      "Loss: 0.024771537631750107\n",
      "Epoch: 988 | MAE Train Loss: 0.024771537631750107 | MAE Test Loss: 0.05781765654683113 \n",
      "Loss: 0.02473772130906582\n",
      "Epoch: 989 | MAE Train Loss: 0.02473772130906582 | MAE Test Loss: 0.05777612328529358 \n",
      "Loss: 0.0247030109167099\n",
      "Epoch: 990 | MAE Train Loss: 0.0247030109167099 | MAE Test Loss: 0.057665932923555374 \n",
      "Loss: 0.024668816477060318\n",
      "Epoch: 991 | MAE Train Loss: 0.024668816477060318 | MAE Test Loss: 0.057624392211437225 \n",
      "Loss: 0.024634484201669693\n",
      "Epoch: 992 | MAE Train Loss: 0.024634484201669693 | MAE Test Loss: 0.05751417949795723 \n",
      "Loss: 0.024599911645054817\n",
      "Epoch: 993 | MAE Train Loss: 0.024599911645054817 | MAE Test Loss: 0.05747263878583908 \n",
      "Loss: 0.024565961211919785\n",
      "Epoch: 994 | MAE Train Loss: 0.024565961211919785 | MAE Test Loss: 0.05736243724822998 \n",
      "Loss: 0.024531058967113495\n",
      "Epoch: 995 | MAE Train Loss: 0.024531058967113495 | MAE Test Loss: 0.05725225806236267 \n",
      "Loss: 0.02449738420546055\n",
      "Epoch: 996 | MAE Train Loss: 0.02449738420546055 | MAE Test Loss: 0.05721070617437363 \n",
      "Loss: 0.024462532252073288\n",
      "Epoch: 997 | MAE Train Loss: 0.024462532252073288 | MAE Test Loss: 0.057100486010313034 \n",
      "Loss: 0.0244284775108099\n",
      "Epoch: 998 | MAE Train Loss: 0.0244284775108099 | MAE Test Loss: 0.057058971375226974 \n",
      "Loss: 0.02439401112496853\n",
      "Epoch: 999 | MAE Train Loss: 0.02439401112496853 | MAE Test Loss: 0.05694875866174698 \n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "epochs = 1000\n",
    "train_loss_values = []\n",
    "test_loss_values = []\n",
    "epoch_count = []\n",
    "for epoch in range(epochs):\n",
    "    modelOne.train()\n",
    "    y_pred = modelOne(X_train)\n",
    "    loss = loss_fn(y_pred,y_train)\n",
    "    print(f\"Loss: {loss}\")\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    modelOne.eval()\n",
    "    with torch.inference_mode():\n",
    "        test_pred = modelOne(X_test)\n",
    "        test_loss = loss_fn(test_pred,y_test.type(torch.float))\n",
    "        print(f\"Epoch: {epoch} | MAE Train Loss: {loss} | MAE Test Loss: {test_loss} \")\n",
    "        train_loss_values.append(loss.detach().numpy())\n",
    "        test_loss_values.append(test_loss.detach().numpy())\n",
    "        epoch_count.append(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "0dea1605-bea0-4aa0-90d9-cee005887bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weights', tensor([0.5788])), ('bias', tensor([0.3509]))])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelOne.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "ec9082e4-66fa-4b52-85c8-4074fb791f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1137af1d0>]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot loss curves\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(epoch_count, train_loss_values, label=\"Train loss\")\n",
    "plt.plot(epoch_count, test_loss_values, label=\"Test loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a1b8ba-a72a-43fa-8e9c-cfb45c892c8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
