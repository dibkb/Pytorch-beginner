{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d69b9c4-5f0e-4ee0-b074-932effef6471",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "841aa20c-25eb-473b-a22f-81324366a513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the data\n",
    "weight = 0.7\n",
    "bias = 0.3\n",
    "\n",
    "X = torch.arange(0,1, 0.02).unsqueeze(dim=1)\n",
    "y = weight * X + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fefdb674-da0a-47b0-a696-f6cad46c7aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 40, 10, 10)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create train/test split\n",
    "train_split = int(0.8 * len(X)) # 80% of data used for training set, 20% for testing \n",
    "X_train, y_train = X[:train_split], y[:train_split]\n",
    "X_test, y_test = X[train_split:], y[train_split:]\n",
    "\n",
    "len(X_train), len(y_train), len(X_test), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2120243a-23d9-4600-809d-b5d2e87b4ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(1,requires_grad = True,dtype = torch.float))\n",
    "        self.bias = nn.Parameter(torch.randn(1,requires_grad = True, dtype = torch.float))\n",
    "        \n",
    "    def forward(self,x:torch.Tensor)->torch.Tensor:\n",
    "        return self.weights * x + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "98ef08b5-aac0-4d4c-a623-7f1301363c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "modelOne = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b15f9f82-1aff-4d7e-8dde-1e382562a0da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([0.3367], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.1288], requires_grad=True)]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(modelOne.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "fee399f3-77bc-4986-a7ac-bec3a5e6659d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up loss function and optimmizer\n",
    "loss_fn = nn.L1Loss()\n",
    "optimizer = torch.optim.SGD(modelOne.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "65e4ece2-7b47-4ba1-b94b-862ae949f5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0004444979131221771\n",
      "Epoch: 0 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 1 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 2 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 3 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 4 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 5 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 6 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 7 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 8 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 9 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 10 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 11 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 12 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 13 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 14 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 15 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 16 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 17 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 18 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 19 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 20 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 21 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 22 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 23 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 24 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 25 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 26 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 27 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 28 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 29 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 30 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 31 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 32 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 33 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 34 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 35 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 36 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 37 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 38 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 39 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 40 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 41 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 42 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 43 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 44 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 45 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 46 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 47 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 48 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 49 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 50 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 51 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 52 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 53 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 54 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 55 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 56 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 57 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 58 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 59 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 60 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 61 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 62 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 63 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 64 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 65 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 66 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 67 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 68 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 69 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 70 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 71 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 72 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 73 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 74 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 75 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 76 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 77 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 78 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 79 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 80 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 81 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 82 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 83 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 84 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 85 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 86 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 87 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 88 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 89 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 90 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 91 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 92 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 93 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 94 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 95 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 96 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 97 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 98 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 99 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 100 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 101 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 102 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 103 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 104 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 105 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 106 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 107 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 108 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 109 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 110 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 111 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 112 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 113 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 114 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 115 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 116 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 117 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 118 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 119 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 120 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 121 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 122 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 123 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 124 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 125 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 126 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 127 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 128 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 129 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 130 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 131 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 132 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 133 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 134 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 135 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 136 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 137 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 138 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 139 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 140 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 141 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 142 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 143 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 144 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 145 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 146 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 147 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 148 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 149 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 150 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 151 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 152 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 153 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 154 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 155 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 156 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 157 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 158 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 159 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 160 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 161 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 162 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 163 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 164 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 165 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 166 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 167 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 168 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 169 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 170 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 171 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 172 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 173 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 174 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 175 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 176 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 177 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 178 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 179 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 180 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 181 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 182 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 183 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 184 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 185 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 186 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 187 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 188 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 189 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 190 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 191 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 192 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 193 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 194 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 195 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 196 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 197 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 198 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 199 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 200 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 201 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 202 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 203 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 204 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 205 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 206 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 207 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 208 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 209 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 210 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 211 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 212 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 213 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 214 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 215 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 216 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 217 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 218 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 219 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 220 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 221 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 222 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 223 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 224 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 225 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 226 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 227 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 228 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 229 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 230 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 231 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 232 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 233 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 234 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 235 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 236 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 237 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 238 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 239 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 240 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 241 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 242 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 243 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 244 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 245 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 246 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 247 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 248 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 249 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 250 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 251 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 252 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 253 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 254 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 255 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 256 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 257 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 258 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 259 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 260 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 261 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 262 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 263 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 264 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 265 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 266 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 267 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 268 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 269 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 270 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 271 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 272 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 273 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 274 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 275 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 276 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 277 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 278 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 279 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 280 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 281 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 282 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 283 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 284 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 285 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 286 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 287 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 288 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 289 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 290 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 291 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 292 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 293 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 294 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 295 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 296 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 297 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 298 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 299 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 300 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 301 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 302 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 303 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 304 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 305 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 306 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 307 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 308 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 309 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 310 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 311 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 312 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 313 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 314 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 315 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 316 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 317 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 318 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 319 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 320 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 321 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 322 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 323 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 324 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 325 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 326 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 327 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 328 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 329 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 330 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 331 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 332 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 333 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 334 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 335 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 336 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 337 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 338 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 339 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 340 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 341 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 342 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 343 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 344 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 345 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 346 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 347 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 348 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 349 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 350 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 351 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 352 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 353 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 354 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 355 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 356 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 357 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 358 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 359 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 360 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 361 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 362 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 363 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 364 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 365 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 366 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 367 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 368 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 369 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 370 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 371 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 372 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 373 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 374 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 375 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 376 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 377 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 378 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 379 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 380 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 381 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 382 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 383 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 384 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 385 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 386 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 387 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 388 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 389 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 390 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 391 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 392 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 393 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 394 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 395 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 396 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 397 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 398 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 399 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 400 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 401 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 402 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 403 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 404 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 405 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 406 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 407 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 408 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 409 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 410 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 411 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 412 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 413 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 414 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 415 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 416 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 417 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 418 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 419 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 420 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 421 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 422 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 423 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 424 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 425 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 426 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 427 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 428 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 429 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 430 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 431 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 432 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 433 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 434 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 435 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 436 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 437 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 438 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 439 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 440 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 441 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 442 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 443 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 444 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 445 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 446 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 447 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 448 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 449 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 450 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 451 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 452 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 453 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 454 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 455 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 456 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 457 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 458 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 459 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 460 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 461 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 462 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 463 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 464 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 465 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 466 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 467 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 468 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 469 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 470 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 471 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 472 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 473 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 474 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 475 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 476 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 477 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 478 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 479 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 480 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 481 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 482 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 483 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 484 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 485 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 486 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 487 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 488 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 489 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 490 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 491 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 492 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 493 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 494 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 495 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 496 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 497 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 498 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 499 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 500 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 501 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 502 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 503 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 504 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 505 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 506 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 507 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 508 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 509 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 510 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 511 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 512 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 513 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 514 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 515 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 516 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 517 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 518 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 519 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 520 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 521 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 522 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 523 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 524 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 525 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 526 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 527 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 528 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 529 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 530 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 531 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 532 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 533 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 534 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 535 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 536 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 537 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 538 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 539 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 540 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 541 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 542 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 543 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 544 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 545 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 546 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 547 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 548 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 549 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 550 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 551 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 552 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 553 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 554 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 555 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 556 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 557 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 558 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 559 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 560 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 561 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 562 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 563 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 564 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 565 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 566 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 567 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 568 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 569 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 570 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 571 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 572 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 573 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 574 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 575 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 576 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 577 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 578 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 579 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 580 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 581 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 582 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 583 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 584 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 585 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 586 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 587 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 588 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 589 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 590 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 591 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 592 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 593 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 594 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 595 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 596 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 597 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 598 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 599 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 600 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 601 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 602 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 603 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 604 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 605 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 606 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 607 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 608 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 609 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 610 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 611 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 612 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 613 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 614 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 615 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 616 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 617 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 618 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 619 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 620 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 621 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 622 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 623 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 624 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 625 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 626 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 627 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 628 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 629 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 630 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 631 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 632 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 633 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 634 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 635 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 636 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 637 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 638 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 639 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 640 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 641 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 642 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 643 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 644 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 645 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 646 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 647 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 648 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 649 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 650 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 651 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 652 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 653 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 654 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 655 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 656 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 657 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 658 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 659 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 660 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 661 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 662 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 663 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 664 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 665 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 666 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 667 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 668 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 669 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 670 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 671 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 672 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 673 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 674 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 675 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 676 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 677 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 678 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 679 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 680 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 681 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 682 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 683 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 684 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 685 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 686 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 687 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 688 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 689 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 690 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 691 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 692 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 693 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 694 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 695 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 696 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 697 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 698 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 699 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 700 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 701 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 702 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 703 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 704 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 705 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 706 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 707 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 708 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 709 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 710 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 711 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 712 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 713 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 714 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 715 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 716 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 717 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 718 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 719 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 720 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 721 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 722 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 723 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 724 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 725 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 726 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 727 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 728 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 729 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 730 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 731 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 732 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 733 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 734 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 735 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 736 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 737 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 738 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 739 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 740 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 741 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 742 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 743 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 744 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 745 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 746 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 747 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 748 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 749 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 750 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 751 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 752 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 753 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 754 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 755 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 756 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 757 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 758 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 759 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 760 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 761 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 762 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 763 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 764 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 765 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 766 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 767 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 768 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 769 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 770 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 771 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 772 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 773 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 774 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 775 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 776 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 777 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 778 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 779 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 780 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 781 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 782 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 783 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 784 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 785 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 786 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 787 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 788 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 789 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 790 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 791 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 792 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 793 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 794 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 795 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 796 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 797 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 798 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 799 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 800 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 801 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 802 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 803 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 804 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 805 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 806 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 807 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 808 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 809 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 810 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 811 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 812 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 813 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 814 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 815 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 816 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 817 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 818 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 819 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 820 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 821 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 822 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 823 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 824 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 825 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 826 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 827 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 828 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 829 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 830 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 831 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 832 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 833 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 834 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 835 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 836 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 837 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 838 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 839 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 840 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 841 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 842 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 843 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 844 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 845 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 846 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 847 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 848 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 849 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 850 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 851 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 852 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 853 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 854 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 855 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 856 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 857 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 858 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 859 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 860 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 861 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 862 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 863 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 864 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 865 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 866 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 867 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 868 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 869 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 870 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 871 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 872 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 873 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 874 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 875 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 876 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 877 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 878 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 879 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 880 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 881 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 882 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 883 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 884 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 885 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 886 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 887 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 888 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 889 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 890 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 891 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 892 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 893 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 894 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 895 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 896 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 897 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 898 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 899 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 900 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 901 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 902 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 903 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 904 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 905 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 906 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 907 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 908 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 909 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 910 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 911 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 912 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 913 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 914 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 915 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 916 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 917 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 918 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 919 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 920 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 921 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 922 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 923 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 924 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 925 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 926 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 927 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 928 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 929 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 930 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 931 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 932 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 933 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 934 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 935 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 936 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 937 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 938 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 939 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 940 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 941 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 942 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 943 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 944 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 945 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 946 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 947 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 948 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 949 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 950 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 951 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 952 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 953 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 954 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 955 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 956 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 957 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 958 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 959 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 960 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 961 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 962 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 963 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 964 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 965 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 966 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 967 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 968 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 969 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 970 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 971 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 972 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 973 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 974 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 975 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 976 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 977 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 978 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 979 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 980 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 981 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 982 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 983 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 984 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 985 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 986 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 987 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 988 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 989 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 990 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 991 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 992 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 993 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 994 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 995 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 996 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 997 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n",
      "Loss: 0.0004444979131221771\n",
      "Epoch: 998 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Loss: 0.0007075957837514579\n",
      "Epoch: 999 | MAE Train Loss: 0.0007075957837514579 | MAE Test Loss: 0.0007690846687182784 \n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    modelOne.train()\n",
    "    y_pred = modelOne(X_train)\n",
    "    loss = loss_fn(y_pred,y_train)\n",
    "    print(f\"Loss: {loss}\")\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    modelOne.eval()\n",
    "    with torch.inference_mode():\n",
    "        test_pred = modelOne(X_test)\n",
    "        test_loss = loss_fn(test_pred,y_test.type(torch.float))\n",
    "        print(f\"Epoch: {epoch} | MAE Train Loss: {loss} | MAE Test Loss: {test_loss} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "0dea1605-bea0-4aa0-90d9-cee005887bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weights', tensor([0.6994])), ('bias', tensor([0.2998]))])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelOne.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad7e3a0-9431-44b9-bcde-d015f02f4cc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
